{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following karpathy's 'Let's build GPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13021.88s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13027.57s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dyld[70397]: Library not loaded: /usr/local/opt/libunistring/lib/libunistring.5.dylib\n",
      "  Referenced from: <47C48650-B755-38AC-AFF0-5FAD3DC27309> /usr/local/Cellar/wget/1.24.5/bin/wget\n",
      "  Reason: tried: '/usr/local/opt/libunistring/lib/libunistring.5.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libunistring/lib/libunistring.5.dylib' (no such file), '/usr/local/opt/libunistring/lib/libunistring.5.dylib' (no such file)\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!mkdir data\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tinyshakespeare') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/tinyshakespeare\n",
      "  input_format: \n",
      "  model_prefix: models/shakespeare_tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: \n",
      "\n",
      "  user_defined_symbols: \n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/tinyshakespeare\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108171\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563798\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33675 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25671\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25671 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12645 obj=11.7019 num_tokens=52717 num_tokens/piece=4.169\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10772 obj=9.46046 num_tokens=53065 num_tokens/piece=4.9262\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8077 obj=9.47102 num_tokens=56344 num_tokens/piece=6.97586\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8069 obj=9.43248 num_tokens=56395 num_tokens/piece=6.98909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6049 obj=9.62524 num_tokens=61716 num_tokens/piece=10.2027\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6049 obj=9.58053 num_tokens=61712 num_tokens/piece=10.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4536 obj=9.85482 num_tokens=68025 num_tokens/piece=14.9967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4536 obj=9.80345 num_tokens=68028 num_tokens/piece=14.9974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3402 obj=10.137 num_tokens=74970 num_tokens/piece=22.037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3402 obj=10.0857 num_tokens=74971 num_tokens/piece=22.0373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2551 obj=10.4554 num_tokens=81942 num_tokens/piece=32.1215\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2551 obj=10.4013 num_tokens=81951 num_tokens/piece=32.125\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1913 obj=10.8732 num_tokens=89431 num_tokens/piece=46.7491\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1913 obj=10.8064 num_tokens=89440 num_tokens/piece=46.7538\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1434 obj=11.3422 num_tokens=96817 num_tokens/piece=67.5153\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1434 obj=11.2617 num_tokens=96830 num_tokens/piece=67.5244\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=11.8572 num_tokens=103715 num_tokens/piece=94.2864\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=11.7728 num_tokens=103744 num_tokens/piece=94.3127\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/shakespeare_tokenizer_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/shakespeare_tokenizer_model.vocab\n",
      "trainer_interface.cc(707) LOG(WARNING) The piece [\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='data/tinyshakespeare',\n",
    "                               model_prefix='models/shakespeare_tokenizer_model',\n",
    "                               vocab_size=1000,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='unigram',\n",
    "                               remove_extra_whitespaces=False,\n",
    "                               user_defined_symbols=[\"\\n\", \"\\r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/shakespeare_tokenizer_model.model')\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  3, 175,  13,  66, 610,  26,  27, 200],\n",
       "         [ 97, 128,  10,   5,  77,  11,  46, 109],\n",
       "         [ 39,  16,  12, 709,  30,   3,   3, 191],\n",
       "         [101, 182,  20, 242,   5,  94, 388, 119]]),\n",
       " tensor([[175,  13,  66, 610,  26,  27, 200,  60],\n",
       "         [128,  10,   5,  77,  11,  46, 109, 130],\n",
       "         [ 16,  12, 709,  30,   3,   3, 191,  57],\n",
       "         [182,  20, 242,   5,  94, 388, 119,  36]]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(sp.encode(text))\n",
    "\n",
    "traindata = data[:int(0.9 * len(data))]\n",
    "testdata = data[int(0.9 * len(data)):]\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(data, device):\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "get_batch(traindata, 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 8])\n",
      " ‚Åá  Wherell meR whom WhoESaveC course\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(embed_dim, head_size)\n",
    "        self.key = nn.Linear(embed_dim, head_size)\n",
    "        self.value = nn.Linear(embed_dim, head_size)\n",
    "\n",
    "    def forward(self, inputs): # (B, T, C)\n",
    "        B, T, C = inputs.shape\n",
    "        q = self.query(inputs) # (B, T, head_size)\n",
    "        k = self.key(inputs) # (B, T, head_size)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        scaled_attn = attn / (self.head_size ** 0.5)\n",
    "\n",
    "        tril = torch.tril(torch.ones((T, T), device=inputs.device))\n",
    "        masked_attn = scaled_attn.masked_fill(tril == 0, float('-inf'))\n",
    "        masked_attn = F.softmax(masked_attn, dim=-1)\n",
    "\n",
    "        v = self.value(inputs)\n",
    "        return masked_attn @ v\n",
    "sample_head = SelfAttentionHead(32, 8)\n",
    "print(sample_head(torch.randn((batch_size, block_size, 32))).shape)\n",
    "\n",
    "class MultiheadedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(embed_dim, embed_dim // num_heads) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, inputs): # (B, T, C=embed_dim)\n",
    "        outputs = torch.cat([h(inputs) for h in self.heads], dim=-1)\n",
    "        return outputs\n",
    "\n",
    "sample_multi_head = MultiheadedSelfAttention(32, 8)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_headed_attn = MultiheadedSelfAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_dim, 4*embed_dim), nn.ReLU(), nn.Linear(4*embed_dim, embed_dim), nn.Dropout())\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outs = inputs + self.multi_headed_attn(self.ln1(inputs))\n",
    "        outs = outs + self.feed_forward(self.ln2(outs))\n",
    "        return outs\n",
    "\n",
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_decoder_layers):\n",
    "        super().__init__()\n",
    "        self.tok_emb_table = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, embed_dim)\n",
    "        self.decoder_blocks = nn.Sequential(*[DecoderBlock(embed_dim, num_heads) for _ in range(num_decoder_layers)])\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs): # inputs: (B, T)\n",
    "        B, T = inputs.shape\n",
    "        tok_embed = self.tok_emb_table(inputs) # (B, T) -> (B, T, C=embed_dim)\n",
    "        pos_embed = self.pos_emb_table(torch.arange(T, , device=inputs.device)) # (T, C=embed_dim)\n",
    "        x = tok_embed + pos_embed  # (B, T, C)\n",
    "        x = self.decoder_blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, C) -> (B, T, vocab_size)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, context, num_tokens): # context: (1, T)\n",
    "        for _ in range(num_tokens):\n",
    "            logits = self(context[:, -block_size:])[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context = torch.cat((context, next_token), dim=1)\n",
    "        return context\n",
    "\n",
    "sample_model = SimpleGPT(vocab_size, 32, 4, 2)\n",
    "print(sp.decode(sample_model.generate(torch.zeros((1,1), dtype=torch.long), 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGPT(\n",
      "  (tok_emb_table): Embedding(1000, 32)\n",
      "  (pos_emb_table): Embedding(8, 32)\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): DecoderBlock(\n",
      "      (multi_headed_attn): MultiheadedSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x SelfAttentionHead(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (multi_headed_attn): MultiheadedSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x SelfAttentionHead(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/10000 [00:00<05:29, 30.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=7.209543228149414 Test Loss=7.182340145111084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 1012/10000 [00:12<03:28, 43.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.925002098083496 Test Loss=4.94713830947876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 2016/10000 [00:24<02:22, 56.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.717228889465332 Test Loss=4.754912376403809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 3022/10000 [00:35<01:37, 71.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.4868574142456055 Test Loss=4.494387626647949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 4017/10000 [00:47<02:25, 41.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.419898986816406 Test Loss=4.452756881713867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5024/10000 [01:00<01:12, 68.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.266473770141602 Test Loss=4.402193069458008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6016/10000 [01:11<00:58, 67.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.143621921539307 Test Loss=4.327620029449463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7017/10000 [01:20<00:53, 55.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.147236347198486 Test Loss=4.222585678100586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8018/10000 [01:32<00:41, 48.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.0140533447265625 Test Loss=4.155172348022461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9015/10000 [01:43<00:17, 57.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.0798444747924805 Test Loss=4.1984543800354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:53<00:00, 88.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.93782114982605 Test Loss=4.172182083129883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from tqdm import tqdm\n",
    "\n",
    "embed_dims = 32\n",
    "num_heads = 4\n",
    "num_decoder_layers = 2\n",
    "eval_iters = 100\n",
    "eval_interval = 1000\n",
    "num_training_iters = 10000\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SimpleGPT(vocab_size, embed_dims, num_heads, num_decoder_layers)\n",
    "model.to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "@torch.no_grad\n",
    "def estimate_loss(dataset):\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    model.eval()\n",
    "    for i in range(eval_iters):\n",
    "        inputs, targets = get_batch(dataset, device)\n",
    "        logits = model(inputs)\n",
    "        B, T, C = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_training_iters)):\n",
    "    inputs, targets = get_batch(traindata, device)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(inputs)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % eval_interval == 0 or i == num_training_iters - 1:\n",
    "        print(f\"Train Loss={estimate_loss(traindata)} Test Loss={estimate_loss(testdata)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚Åá , my doth; and made.\n",
      "I rellwe to be matterly like.\n",
      "\n",
      "MENENIUS:\n",
      "Aut but do the soul\n",
      "lod.\n",
      "\n",
      "IISfst fortune not ofit was from unmury; paburculd-hinsred not off, his power;\n",
      "Cr noble things? stand--bers outdr plock slavees, marriage of reter and mioldvus val upon int,\n",
      "Dive by the draw!\n",
      "His light with your soulwic, and my country:\n",
      "Orough still but this away 'KING be knows?LEONTES:\n",
      "My hit with thou or! Come more home; sir!\n",
      "\n",
      "ISomeal name:\n",
      "And in exil times.\n",
      "Must of this on heavens lieter in himself.\n",
      "Master the think'd sun sleep\n",
      "The as allter but my sovereign back alls. well the shistanto praclition that lo's V mightk of Gloucester: des Lggehils depance timek my titlegis,\n",
      "That is'lled go down.\n",
      "\n",
      "MENEIF YORK:m you newands is as that so this fent and her may modumms's bohot more were not unt queen of abertutnewilesed,\n",
      "Te care\n",
      "Yo, I live are cake do believe the prince.\n",
      "What! Gors it.\n",
      "\n",
      "WARWICK:\n",
      "Mairus\n",
      "Thereret even Lucentios here very\n",
      "s dond on\n",
      "YORortspish. For\n",
      "Far, when you I showd woesst and beenceter grantfuld of theies;\n",
      "Arest my end and so poor:\n",
      "And thee it, a nemityW hered yielded being comes and the euis.\n",
      "\n",
      "\n",
      "BENVOLIO:\n",
      "Therey father?\n",
      "\n",
      "BNIUS:\n",
      "Aout you one we\n",
      "LANDUS:\n",
      "Ah I, I can, ifs thee isll to he dangerick. Camillo your amk; yous gicleen uplesswicwt shallld out.\n",
      "\n",
      "GCCAWherNCE:\n",
      "Not boize in bubuldour, whom them\n",
      "That Rall will me:\n",
      "How is all on offing\n",
      "Ao accusure but a word to parked\n",
      "Notot, patience I are another;.\n",
      "\n",
      "DUEENES:\n",
      "My, sir.Plaerly kpleniolsen me cannot that we are:\n",
      "The musta breath!\n",
      "How lookrlly it, and\n",
      "Wileforless at his\n",
      "Ant out, inn I were Cortrasy reprosion of unbe, then mis'tigsal better.\n",
      "Mornhest speak, I should take I meanven. Ha\n",
      "\n",
      "MHIY:\n",
      "Buest is well, say again she he part hath hereas hate proder in turn blood:\n",
      "Atch him they to joy!\n",
      "\n",
      "The man-- Citizen:\n",
      "I go.\n",
      "\n",
      "MENENGE he IS\n",
      "Ited.\n",
      "Look strewears is, a an faithgdu!\n",
      "\n",
      "Anot mistress of the posted.\n",
      "Epemage and demps is friend soifbo,\n",
      "Brells henced thee.\n",
      "\n",
      "Gour is a subject disces here,\n",
      "and and roventhery patain us and the friar: great the mothers of clice thereforll hims it ble to the young chuforsik,\n",
      "As mean me.\n",
      "\n",
      "I may set masters the care; your shoforesk but alone; and he be os.\n",
      "\n",
      "DAMAY:\n",
      "Hhen: him no more woves dismbves\n",
      "Thucustful timed of or light to beholds that no tad in years of duke, you by thine, and her die to his.\n",
      "\n",
      "GLOUCESTENE\n"
     ]
    }
   ],
   "source": [
    "print(sp.decode(model.generate(torch.zeros((1,1), dtype=torch.long, device = device), 1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
