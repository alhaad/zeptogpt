{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following karpathy's 'Let's build GPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  2729k      0 --:--:-- --:--:-- --:--:-- 2736k\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tinyshakespeare') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/tinyshakespeare\n",
      "  input_format: \n",
      "  model_prefix: models/shakespeare_tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: \n",
      "\n",
      "  user_defined_symbols: \n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/tinyshakespeare\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108171\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563798\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33675 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25671\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25671 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12645 obj=11.7019 num_tokens=52717 num_tokens/piece=4.169\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10772 obj=9.46046 num_tokens=53065 num_tokens/piece=4.9262\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8077 obj=9.47102 num_tokens=56344 num_tokens/piece=6.97586\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8069 obj=9.43248 num_tokens=56395 num_tokens/piece=6.98909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6049 obj=9.62524 num_tokens=61716 num_tokens/piece=10.2027\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6049 obj=9.58053 num_tokens=61712 num_tokens/piece=10.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4536 obj=9.85482 num_tokens=68025 num_tokens/piece=14.9967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4536 obj=9.80345 num_tokens=68028 num_tokens/piece=14.9974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3402 obj=10.137 num_tokens=74970 num_tokens/piece=22.037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3402 obj=10.0857 num_tokens=74971 num_tokens/piece=22.0373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2551 obj=10.4554 num_tokens=81942 num_tokens/piece=32.1215\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2551 obj=10.4013 num_tokens=81951 num_tokens/piece=32.125\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1913 obj=10.8732 num_tokens=89431 num_tokens/piece=46.7491\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1913 obj=10.8064 num_tokens=89440 num_tokens/piece=46.7538\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1434 obj=11.3422 num_tokens=96817 num_tokens/piece=67.5153\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1434 obj=11.2617 num_tokens=96830 num_tokens/piece=67.5244\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=11.8572 num_tokens=103715 num_tokens/piece=94.2864\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=11.7728 num_tokens=103744 num_tokens/piece=94.3127\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/shakespeare_tokenizer_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/shakespeare_tokenizer_model.vocab\n",
      "trainer_interface.cc(707) LOG(WARNING) The piece [\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='data/tinyshakespeare',\n",
    "                               model_prefix='models/shakespeare_tokenizer_model',\n",
    "                               vocab_size=1000,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='unigram',\n",
    "                               remove_extra_whitespaces=False,\n",
    "                               user_defined_symbols=[\"\\n\", \"\\r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/shakespeare_tokenizer_model.model')\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  3, 175,  13,  66, 610,  26,  27, 200],\n",
       "         [ 97, 128,  10,   5,  77,  11,  46, 109],\n",
       "         [ 39,  16,  12, 709,  30,   3,   3, 191],\n",
       "         [101, 182,  20, 242,   5,  94, 388, 119]]),\n",
       " tensor([[175,  13,  66, 610,  26,  27, 200,  60],\n",
       "         [128,  10,   5,  77,  11,  46, 109, 130],\n",
       "         [ 16,  12, 709,  30,   3,   3, 191,  57],\n",
       "         [182,  20, 242,   5,  94, 388, 119,  36]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(sp.encode(text))\n",
    "\n",
    "traindata = data[:int(0.9 * len(data))]\n",
    "testdata = data[int(0.9 * len(data)):]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(data, device, batch_size, block_size):\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "get_batch(traindata, 'cpu', 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 8])\n",
      " ‚Åá  Wherell meR whom WhoESaveC course\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(embed_dim, head_size)\n",
    "        self.key = nn.Linear(embed_dim, head_size)\n",
    "        self.value = nn.Linear(embed_dim, head_size)\n",
    "\n",
    "    def forward(self, inputs): # (B, T, C)\n",
    "        B, T, C = inputs.shape\n",
    "        q = self.query(inputs) # (B, T, head_size)\n",
    "        k = self.key(inputs) # (B, T, head_size)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        scaled_attn = attn / (self.head_size ** 0.5)\n",
    "\n",
    "        tril = torch.tril(torch.ones((T, T), device=inputs.device))\n",
    "        masked_attn = scaled_attn.masked_fill(tril == 0, float('-inf'))\n",
    "        masked_attn = F.softmax(masked_attn, dim=-1)\n",
    "\n",
    "        v = self.value(inputs)\n",
    "        return masked_attn @ v\n",
    "sample_head = SelfAttentionHead(32, 8)\n",
    "print(sample_head(torch.randn((batch_size, block_size, 32))).shape)\n",
    "\n",
    "class MultiheadedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(embed_dim, embed_dim // num_heads) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, inputs): # (B, T, C=embed_dim)\n",
    "        outputs = torch.cat([h(inputs) for h in self.heads], dim=-1)\n",
    "        return outputs\n",
    "\n",
    "sample_multi_head = MultiheadedSelfAttention(32, 8)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_headed_attn = MultiheadedSelfAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_dim, 4*embed_dim), nn.ReLU(), nn.Linear(4*embed_dim, embed_dim), nn.Dropout())\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outs = inputs + self.multi_headed_attn(self.ln1(inputs))\n",
    "        outs = outs + self.feed_forward(self.ln2(outs))\n",
    "        return outs\n",
    "\n",
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_decoder_layers):\n",
    "        super().__init__()\n",
    "        self.tok_emb_table = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, embed_dim)\n",
    "        self.decoder_blocks = nn.Sequential(*[DecoderBlock(embed_dim, num_heads) for _ in range(num_decoder_layers)])\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs): # inputs: (B, T)\n",
    "        B, T = inputs.shape\n",
    "        tok_embed = self.tok_emb_table(inputs) # (B, T) -> (B, T, C=embed_dim)\n",
    "        pos_embed = self.pos_emb_table(torch.arange(T, device=inputs.device)) # (T, C=embed_dim)\n",
    "        x = tok_embed + pos_embed  # (B, T, C)\n",
    "        x = self.decoder_blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, C) -> (B, T, vocab_size)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, context, num_tokens): # context: (1, T)\n",
    "        for _ in range(num_tokens):\n",
    "            logits = self(context[:, -block_size:])[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context = torch.cat((context, next_token), dim=1)\n",
    "        return context\n",
    "\n",
    "sample_model = SimpleGPT(vocab_size, 32, 4, 2)\n",
    "print(sp.decode(sample_model.generate(torch.zeros((1,1), dtype=torch.long), 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGPT(\n",
      "  (tok_emb_table): Embedding(1000, 32)\n",
      "  (pos_emb_table): Embedding(8, 32)\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): DecoderBlock(\n",
      "      (multi_headed_attn): MultiheadedSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x SelfAttentionHead(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (multi_headed_attn): MultiheadedSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x SelfAttentionHead(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/10000 [00:00<11:22, 14.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=7.270721435546875 Test Loss=7.286463737487793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 1018/10000 [00:12<02:53, 51.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=5.060835838317871 Test Loss=4.926959991455078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 2019/10000 [00:23<02:20, 56.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.615677356719971 Test Loss=4.776318550109863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 3020/10000 [00:34<02:02, 57.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.535714626312256 Test Loss=4.566429138183594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 4017/10000 [00:44<01:48, 55.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.2885422706604 Test Loss=4.4173736572265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5014/10000 [00:54<01:14, 67.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.27218770980835 Test Loss=4.333480358123779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6021/10000 [01:04<00:56, 69.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.175124168395996 Test Loss=4.310807704925537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7016/10000 [01:14<00:47, 62.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.285872459411621 Test Loss=4.289434432983398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8021/10000 [01:23<00:31, 62.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.966357707977295 Test Loss=4.185142993927002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9014/10000 [01:33<00:14, 68.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.032195091247559 Test Loss=4.21633768081665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:43<00:00, 96.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.9990339279174805 Test Loss=4.194402694702148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from tqdm import tqdm\n",
    "\n",
    "embed_dims = 32\n",
    "num_heads = 4\n",
    "num_decoder_layers = 2\n",
    "eval_iters = 100\n",
    "eval_interval = 1000\n",
    "num_training_iters = 10000\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SimpleGPT(vocab_size, embed_dims, num_heads, num_decoder_layers)\n",
    "model.to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "@torch.no_grad\n",
    "def estimate_loss(dataset):\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    model.eval()\n",
    "    for i in range(eval_iters):\n",
    "        inputs, targets = get_batch(dataset, device, batch_size, block_size)\n",
    "        logits = model(inputs)\n",
    "        B, T, C = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_training_iters)):\n",
    "    inputs, targets = get_batch(traindata, device, batch_size, block_size)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(inputs)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % eval_interval == 0 or i == num_training_iters - 1:\n",
    "        print(f\"Train Loss={estimate_loss(traindata)} Test Loss={estimate_loss(testdata)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚Åá ! Aleads;\n",
      "Cloteet him Romeo'er:\n",
      "Bygainsts ach:\n",
      "Toant prahing me wedid\n",
      "The suitan fe against music, his this lall to this lady\n",
      "rie, now, and give votoing, is as these Your as you are stay in chays that.\n",
      "\n",
      "HASTINGUDS:\n",
      "Narven againe Mixoughsight on worth:\n",
      "When the sea myselfs on.\n",
      "O prince me; and pilly lie' with gravely.\n",
      "\n",
      "BRUTUS:\n",
      "Whis such his a maids; and samy is conliseleceue\n",
      "That I warrant your mi happy heart hand, and report,\n",
      "And Servingmanut should your kind'll\n",
      "th-thurkalth-likeiono\n",
      "Andows cu scent.\n",
      "Ans no igenty fool,\n",
      "Raseit in allor forcuk to progoows with usaration.\n",
      "\n",
      "Forite would IV:\n",
      "Nip worthut bid his no you\n",
      "Wo in her ending'dicenred I more be it:\n",
      "Is points:\n",
      "For leaves ing's Da thems:\n",
      "Pry that offices 'S:\n",
      "O, though not the win, is wor thy from?\n",
      "Sar, If upon thy worth flemen long me;\n",
      "Thectterwup.\n",
      "\n",
      "KING RICHARD, strong mistheted and the heartern\n",
      "Firpon which he have the gosesccuution but with the world; who' spion't'dfulisher'd your sengoutend since powers go,\n",
      "Whousread our fathers, though, and be that show world.\n",
      "Go's\n",
      "We-urechine\n",
      "geow that the whens in bantion that to, Cent, likes and tell,\n",
      "If you, and your honour ca?\n",
      "A that spoke!\n",
      "Ware a grave do is\n",
      "Whatcept is the honour?\n",
      "Blk as your general, thatself, by me would toineembed should you;, Aufidius of the first lead seemte I I'th,\n",
      "What the rus lo is the world of the king.\n",
      "W woe!\n",
      "An the dimorrow too!\n",
      "I here for you,\n",
      "Thelor and allk is a ears meounds?\n",
      "\n",
      "SABETH:\n",
      "Whatie' by goverster, that is now it itA the proven every quence and sop's that hither is sporbal' me.\n",
      "\n",
      "FUCIO:\n",
      "The hath but lord;\n",
      "The Wiful wient to me.\n",
      "A-Evun stvorse in his samaphata death:\n",
      "Yo repving, a horse, in a woman not\n",
      "bury, the chalett,\n",
      "Simk to thee, do and tongue noblely thou reg,\n",
      "Yy can posar at qulegi po.\n",
      "\n",
      "SABELLSINAIOLA:\n",
      "Nhance no repidward\n",
      "And with suptens uchorience ru but writ, and Juliet\n",
      "Wichough Bolingbroke,\n",
      "Wey moldgheondion;\n",
      "\n",
      "All, the whatoves in you't thou power I remember their noble by man;\n",
      "O,VOLse no gtorening their up\n",
      "Naris together of itnous sorrow's\n",
      "perlESo't\n",
      "Citake saird upon what a wiltad up that la commanderis win cranened a other prisonthat,\n",
      "Of enough!VOLUMBERLIDUKE VINCE:\n",
      "Wowring kulpeble Merousary, unst, in slave knowers, detlaugh at fly aine your whichs\n",
      "And may and\n",
      "sure you and you are be he fear: for the service.\n",
      "\n",
      "ROMEO:\n",
      "H\n"
     ]
    }
   ],
   "source": [
    "print(sp.decode(model.generate(torch.zeros((1,1), dtype=torch.long, device = device), 1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
