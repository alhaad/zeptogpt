{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Shakespeare Language Model\n",
    "\n",
    "GPT-like language model that is trained on the tinyshakespeare dataset. This notebook was written while following Karpathy's 'Let's build GPT' vide. The only notable difference is the use of SentencePiece tokenization instead of a character level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  7689k      0 --:--:-- --:--:-- --:--:-- 7725k\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tinyshakespeare') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/tinyshakespeare\n",
      "  input_format: \n",
      "  model_prefix: models/shakespeare_tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: \n",
      "\n",
      "  user_defined_symbols: \n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/tinyshakespeare\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108171\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563798\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33675 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25671\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25671 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12645 obj=11.7019 num_tokens=52717 num_tokens/piece=4.169\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10772 obj=9.46046 num_tokens=53065 num_tokens/piece=4.9262\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8077 obj=9.47102 num_tokens=56344 num_tokens/piece=6.97586\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8069 obj=9.43248 num_tokens=56395 num_tokens/piece=6.98909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6049 obj=9.62524 num_tokens=61716 num_tokens/piece=10.2027\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6049 obj=9.58053 num_tokens=61712 num_tokens/piece=10.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4536 obj=9.85482 num_tokens=68025 num_tokens/piece=14.9967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4536 obj=9.80345 num_tokens=68028 num_tokens/piece=14.9974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3402 obj=10.137 num_tokens=74970 num_tokens/piece=22.037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3402 obj=10.0857 num_tokens=74971 num_tokens/piece=22.0373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2551 obj=10.4554 num_tokens=81942 num_tokens/piece=32.1215\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2551 obj=10.4013 num_tokens=81951 num_tokens/piece=32.125\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1913 obj=10.8732 num_tokens=89431 num_tokens/piece=46.7491\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1913 obj=10.8064 num_tokens=89440 num_tokens/piece=46.7538\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1434 obj=11.3422 num_tokens=96817 num_tokens/piece=67.5153\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1434 obj=11.2617 num_tokens=96830 num_tokens/piece=67.5244\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=11.8572 num_tokens=103715 num_tokens/piece=94.2864\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=11.7728 num_tokens=103744 num_tokens/piece=94.3127\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/shakespeare_tokenizer_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/shakespeare_tokenizer_model.vocab\n",
      "trainer_interface.cc(707) LOG(WARNING) The piece [\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='data/tinyshakespeare',\n",
    "                               model_prefix='models/shakespeare_tokenizer_model',\n",
    "                               vocab_size=1000,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='unigram',\n",
    "                               remove_extra_whitespaces=False,\n",
    "                               user_defined_symbols=[\"\\n\", \"\\r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/shakespeare_tokenizer_model.model')\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([[  3,  61,  58,  56, 119,  55,  25,  86],\n",
      "       [  7,   3,  89, 146,  41, 431, 283, 349],\n",
      "       [  3,   3, 360,  61,  89, 111, 599, 122],\n",
      "       [ 72, 125, 170,  22,  25,   7, 102, 356]], dtype=int32), Array([[ 61,  58,  56, 119,  55,  25,  86,   7],\n",
      "       [  3,  89, 146,  41, 431, 283, 349,   5],\n",
      "       [  3, 360,  61,  89, 111, 599, 122, 126],\n",
      "       [125, 170,  22,  25,   7, 102, 356,  94]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "data = jnp.array(sp.encode(text))\n",
    "\n",
    "traindata = data[:int(0.9 * len(data))]\n",
    "testdata = data[int(0.9 * len(data)):]\n",
    "\n",
    "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
    "\n",
    "@partial(jax.jit, static_argnames=['batch_size, block_size'])\n",
    "def get_batch(key, data, batch_size, block_size):\n",
    "    batch_size=4\n",
    "    block_size=8\n",
    "    ix = jax.random.randint(key, shape=(batch_size, 1), minval=0, maxval=len(data) - block_size)\n",
    "    x = dynamic_slice_vmap(data, ix, (block_size,))\n",
    "    y = dynamic_slice_vmap(data, ix + 1, (block_size,))\n",
    "    return x, y\n",
    "\n",
    "key = jax.random.key(1337)\n",
    "for _ in range(1):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    print(get_batch(key, traindata, 4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeptogpt\n",
    "import importlib\n",
    "\n",
    "importlib.reload(zeptogpt)\n",
    "\n",
    "from zeptogpt.model import GPT\n",
    "from zeptogpt.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/10000 [00:05<54:08,  3.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=7.338044166564941 Test Loss=7.307309627532959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1033/10000 [00:09<00:45, 198.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.81251859664917 Test Loss=4.781274795532227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2073/10000 [00:12<00:26, 302.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.3535614013671875 Test Loss=4.4693522453308105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3063/10000 [00:14<00:25, 271.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.334170818328857 Test Loss=4.40308141708374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4052/10000 [00:17<00:18, 329.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.162240028381348 Test Loss=4.308862209320068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5061/10000 [00:19<00:17, 279.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.128535270690918 Test Loss=4.30137825012207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6075/10000 [00:22<00:12, 325.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.9623284339904785 Test Loss=4.1711297035217285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7069/10000 [00:24<00:08, 361.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.969635009765625 Test Loss=4.069582939147949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8090/10000 [00:26<00:05, 321.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.937488317489624 Test Loss=4.138076305389404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 9059/10000 [00:29<00:03, 249.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.022124290466309 Test Loss=4.181920528411865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:32<00:00, 312.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.875575542449951 Test Loss=4.13692569732666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import functools\n",
    "\n",
    "embed_dims = 32\n",
    "num_heads = 4\n",
    "num_decoder_layers = 2\n",
    "eval_iters = 100\n",
    "eval_interval = 1000\n",
    "num_training_iters = 10000\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "key = jax.random.key(1337)\n",
    "model = GPT(vocab_size, block_size, embed_dims, num_heads, num_decoder_layers)\n",
    "params = model.init(key, jnp.ones((1, block_size), dtype=jnp.int32))\n",
    "optimizer = optax.adamw(learning_rate=0.002)\n",
    "\n",
    "# Create training state\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimizer\n",
    ")\n",
    "loss_fn = lambda logits, targets: optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
    "train_batch_fn = functools.partial(get_batch, data=traindata, batch_size=batch_size, block_size=block_size)\n",
    "test_batch_fn = functools.partial(get_batch, data=testdata, batch_size=batch_size, block_size=block_size)\n",
    "trainer = Trainer(state, loss_fn, train_batch_fn, test_batch_fn, eval_iters, eval_interval, num_training_iters)\n",
    "state = trainer.train(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d forward they come, cousin at her\n",
      "My, or age.\n",
      "ISer'sish, you are well to,\n",
      "I have brield slain\n",
      "I amre, in honour which kned with, see, the coplave you Kate, I have exvy, it should follow Henry?\n",
      "Give\n",
      "OKE VINCENTIO cres curst against thee fromet Cise inweashers your full.\n",
      "\n",
      "KING EDWARD I's thou' how I here:\n",
      "To.\n",
      "Hein'pose my royal so now,\n",
      "\n",
      "I, strership came.BRUTUS: come cold,\n",
      "DUCCANO you, and kill him me and myness, and, and long to cubar ppostercience be then I knows, when itfckselamey k and deck, king.\n",
      "Hhereas partter.\n",
      "Gogo soun you rather trg with'd,\n",
      "ARuituse ho?\n",
      "Are to unt:\n",
      "KING RICHARD I learn of thy way theic no?\n",
      "\n",
      "BUCAMILLO:\n",
      "\n",
      "f Cors somein, your honour that I,\n",
      "BUKE ONow! should this news a spow'd exLL against him asin, but Here him,\n",
      "Are.\n",
      "\n",
      "I:\n",
      "As! holy are not,\n",
      "By. Good,\n",
      "'sed pom a old serly\n",
      "UKE VINCENTILAU'ss's,\n",
      "I EDWARDLAURENCENTIO:\n",
      "chiin some de's\n",
      "NO:\n",
      "Wersion tongue?\n",
      "Hans more I provant had spey say nots.\n",
      "By; and know by France nothing\n",
      "\n",
      "I am from a her with thy face.\n",
      "Ho is yet yetning yield: rand\n",
      "Equine,\n",
      "\n",
      "O:\n",
      "\n",
      "Be,.\n",
      "\n",
      "Gon'd's deserve, I could have coi of born know atossipept you curse wa, she cotiing great Ap overs, we ining of my king'd no.\n",
      "SOMEORK: we or I should lord.\n",
      "thluble coing;\n",
      "PRUM:\n",
      "w willMIOV: but s three!\n",
      "I should, but you have my father,\n",
      "Hint for all.\n",
      "Lorsour the tell him needso not:\n",
      "IL Warwick?\n",
      "\n",
      "Yuker afterthout of shame:\n",
      "O:\n",
      "Suun will been tovles fore?\n",
      "F YO:\n",
      "BLORD heeuns in his giequmentit thou we fol, standing;\n",
      "Why.\n",
      "WARWICK:\n",
      "A mean, no down out?\n",
      "My s the striders.\n",
      "Where there how I we'shar that godure\n",
      "PBRortales; that noble than wes woutd innience she would no the king, I pro Aperos and hoies from his a pain of peace\n",
      "O artise ed too brer I have been nature should s myNGBRO:\n",
      "TBuld laen anstor Heoes from heavening,\n",
      "HENRY Burid;\n",
      "\n",
      "I palcemp\n",
      "ARENCENTIing ss Ce of further fornne'd from yours unly,\n",
      "Asour whouful ed; and love.\n",
      "I'diever'd I doubt we unmorrow her;\n",
      "Thure, and, and sweetss ferans inhe, I could bre felce for then it;\n",
      "I can bey,\n",
      "FAs old with you may be he.\n",
      "\n",
      "H:\n",
      "KING EDWARD I in dim that?\n",
      "IAco\n",
      "KING EDWARD I is two let me,\n",
      "I:\n",
      "Twaking, and come withich: beer!\n",
      "\n",
      "ESSow por' cing.\n",
      "TARENCENTIages onance,\n",
      "And\n",
      "KING RICHARD ass at that.\n",
      "WM:\n",
      "An faith?\n",
      "Sul more,\n",
      "Boste\n",
      "I will be incaciss exlir em?\n",
      "As?\n",
      "YLYC\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(1337)\n",
    "model = GPT(vocab_size, block_size, embed_dims, num_heads, num_decoder_layers)\n",
    "print(sp.decode(model.generate(key, state.params, jnp.ones((1, 1), dtype=jnp.int32) * 13, 1000)[:, 0, 0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
