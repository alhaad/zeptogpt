{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Shakespeare Language Model\n",
    "\n",
    "GPT-like language model that is trained on the tinyshakespeare dataset. This notebook was written while following Karpathy's 'Let's build GPT' vide. The only notable difference is the use of SentencePiece tokenization instead of a character level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  2638k      0 --:--:-- --:--:-- --:--:-- 2643k\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from zeptogpt.model import SimpleGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tinyshakespeare') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/tinyshakespeare\n",
      "  input_format: \n",
      "  model_prefix: models/shakespeare_tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: \n",
      "\n",
      "  user_defined_symbols: \n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/tinyshakespeare\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108171\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563798\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33675 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25671\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25671 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12645 obj=11.7019 num_tokens=52717 num_tokens/piece=4.169\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10772 obj=9.46046 num_tokens=53065 num_tokens/piece=4.9262\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8077 obj=9.47102 num_tokens=56344 num_tokens/piece=6.97586\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8069 obj=9.43248 num_tokens=56395 num_tokens/piece=6.98909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6049 obj=9.62524 num_tokens=61716 num_tokens/piece=10.2027\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6049 obj=9.58053 num_tokens=61712 num_tokens/piece=10.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4536 obj=9.85482 num_tokens=68025 num_tokens/piece=14.9967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4536 obj=9.80345 num_tokens=68028 num_tokens/piece=14.9974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3402 obj=10.137 num_tokens=74970 num_tokens/piece=22.037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3402 obj=10.0857 num_tokens=74971 num_tokens/piece=22.0373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2551 obj=10.4554 num_tokens=81942 num_tokens/piece=32.1215\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2551 obj=10.4013 num_tokens=81951 num_tokens/piece=32.125\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1913 obj=10.8732 num_tokens=89431 num_tokens/piece=46.7491\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1913 obj=10.8064 num_tokens=89440 num_tokens/piece=46.7538\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1434 obj=11.3422 num_tokens=96817 num_tokens/piece=67.5153\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1434 obj=11.2617 num_tokens=96830 num_tokens/piece=67.5244\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=11.8572 num_tokens=103715 num_tokens/piece=94.2864\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=11.7728 num_tokens=103744 num_tokens/piece=94.3127\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/shakespeare_tokenizer_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/shakespeare_tokenizer_model.vocab\n",
      "trainer_interface.cc(707) LOG(WARNING) The piece [\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='data/tinyshakespeare',\n",
    "                               model_prefix='models/shakespeare_tokenizer_model',\n",
    "                               vocab_size=1000,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='unigram',\n",
    "                               remove_extra_whitespaces=False,\n",
    "                               user_defined_symbols=[\"\\n\", \"\\r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/shakespeare_tokenizer_model.model')\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  3, 175,  13,  66, 610,  26,  27, 200],\n",
       "         [ 97, 128,  10,   5,  77,  11,  46, 109],\n",
       "         [ 39,  16,  12, 709,  30,   3,   3, 191],\n",
       "         [101, 182,  20, 242,   5,  94, 388, 119]]),\n",
       " tensor([[175,  13,  66, 610,  26,  27, 200,  60],\n",
       "         [128,  10,   5,  77,  11,  46, 109, 130],\n",
       "         [ 16,  12, 709,  30,   3,   3, 191,  57],\n",
       "         [182,  20, 242,   5,  94, 388, 119,  36]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(sp.encode(text))\n",
    "\n",
    "traindata = data[:int(0.9 * len(data))]\n",
    "testdata = data[int(0.9 * len(data)):]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(data, device, batch_size, block_size):\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "get_batch(traindata, 'cpu', 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGPT(\n",
      "  (tok_emb_table): Embedding(1000, 32)\n",
      "  (pos_emb_table): Embedding(8, 32)\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): DecoderBlock(\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadedSelfAttention(\n",
      "        (c_attn): Linear(in_features=32, out_features=96, bias=True)\n",
      "        (c_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadedSelfAttention(\n",
      "        (c_attn): Linear(in_features=32, out_features=96, bias=True)\n",
      "        (c_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=32, out_features=1000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 21/10000 [00:00<02:40, 62.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=7.0566182136535645 Test Loss=7.075974941253662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1036/10000 [00:05<00:54, 165.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=5.057060718536377 Test Loss=4.96154260635376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2027/10000 [00:10<00:48, 165.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.6959309577941895 Test Loss=4.719968318939209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3044/10000 [00:15<00:38, 179.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.432583808898926 Test Loss=4.549083709716797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4041/10000 [00:20<00:40, 145.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.3404436111450195 Test Loss=4.444079399108887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5041/10000 [00:24<00:27, 177.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.250292778015137 Test Loss=4.368217468261719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6035/10000 [00:29<00:23, 172.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.156818389892578 Test Loss=4.302825927734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7028/10000 [00:33<00:17, 169.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.123233318328857 Test Loss=4.143256664276123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8032/10000 [00:39<00:21, 91.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.9391846656799316 Test Loss=4.1493916511535645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9034/10000 [00:44<00:05, 176.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.969677448272705 Test Loss=4.232929706573486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:48<00:00, 206.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.024553298950195 Test Loss=4.1811370849609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from tqdm import tqdm\n",
    "\n",
    "embed_dims = 32\n",
    "num_heads = 4\n",
    "num_decoder_layers = 2\n",
    "eval_iters = 100\n",
    "eval_interval = 1000\n",
    "num_training_iters = 10000\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SimpleGPT(vocab_size, embed_dims, block_size, num_heads, num_decoder_layers)\n",
    "model.to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "@torch.no_grad\n",
    "def estimate_loss(dataset):\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    model.eval()\n",
    "    for i in range(eval_iters):\n",
    "        inputs, targets = get_batch(dataset, device, batch_size, block_size)\n",
    "        logits = model(inputs)\n",
    "        B, T, C = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_training_iters)):\n",
    "    inputs, targets = get_batch(traindata, device, batch_size, block_size)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(inputs)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % eval_interval == 0 or i == num_training_iters - 1:\n",
    "        print(f\"Train Loss={estimate_loss(traindata)} Test Loss={estimate_loss(testdata)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sowtlabssked by three that am thy married\n",
      "Theth mine, he graveer I but see'chent-bome ed:\n",
      "Nor is have heaven not finrigeage but makes'll think\n",
      "EEN EL:\n",
      "Mend, when I would op'th bad-bly.\n",
      "\n",
      "ANTIGONTHSingan fighticail.\n",
      "Mhe me there now vies laveriess that have fosemonk him and I am,\n",
      "three rather insolel?\n",
      "\n",
      "NORK: for the grace Lord you.\n",
      "\n",
      "ESCALUS:\n",
      "I hast to set and\n",
      "ARS answer,\n",
      "Second subjects, thou shalt soon lety\n",
      "I not nothing, I bulueth of call,\n",
      "I 'And sovereign more wordly to'd of my daughter to themer,\n",
      "Neaster a sateice\n",
      "Second huitizeplastutonceal me for most\n",
      "Thoal kind what,\n",
      "Loundantly must appeasoedfulpel stillgickerk's me to labarvsumer wit must to puten sweet fe.\n",
      "\n",
      "PNEV:\n",
      "Do that fortdthers oby, much vainrond life.\n",
      "\n",
      "YomRUT ELIZ:\n",
      "Sover meuld it like adoorem's.\n",
      "Mel you bid that it be here, howtimeg and this now.\n",
      "\n",
      "H\n",
      "ASIOLERRENCE:\n",
      "Head die tend wit, but have give of this abasse; see the Lady here, and mother mech to reper att takes and semstiT:\n",
      "I veryt our country, joundsit\n",
      "Bus lieleten us you too\n",
      "Ad, let thy shacces!KE:\n",
      "My her respectd feence,--\n",
      "I am knowat is prs's rpes virtue Aad, that name, to my subject'd's now, andeign you born,\n",
      "And 'tis their good ha to propslialon Montague your dedddader? twenty if strength that unomikss derarlesate,\n",
      "ise solant long? do here\n",
      "\n",
      "We be God, to the peoplender'er\n",
      "BUCIO:\n",
      "O:\n",
      "Wh Reads,\n",
      "Comes shall' the grace.\n",
      "The prince soul, then at my ladys he now.\n",
      "\n",
      "POLICALUS:\n",
      "Rre! but the word, gentle Gloucester's me all sea Edward per'k to the yield. for propohension.\n",
      "\n",
      "ARIUS:\n",
      "I.\n",
      "\n",
      "Sirst W'ate:\n",
      "Ford acnceath myNEe and in your mothermans, and\n",
      "fity me is yet be queen the subjects.\n",
      "\n",
      "Oashat, God you;\n",
      "Opost me, and the hour conhatpe,\n",
      "And III:\n",
      "Hewtain prodounes.\n",
      "\n",
      "K poison, let their she must make yet you should is\n",
      "En be your grace Leemisest deserve.\n",
      "\n",
      "CLAUIUS:\n",
      "A never Come mayd him des give mounp.\n",
      "PSAF these helprfoing exer bed of the trightd.\n",
      "\n",
      "I mean to us: thee to wonder impant.\n",
      "\n",
      "Cainge fity the brar-pandorpver in my heart and no done.\n",
      "\n",
      "TheUKE:\n",
      "Thich hastve these my more;\n",
      "To still that follow exnMy my be less,\n",
      "And Bce one to ctlednutcan but\n",
      "Ock, 'tisbde should wew of great framas:\n",
      "Sies, scer of me have words.\n",
      "\n",
      "IUS:\n",
      "Ael's gone mees\n",
      "TOr first it, should thou ach on I\n",
      "Nord YORK:\n",
      "And present the laned by imp rebest,\n",
      "ilector me show Thouly ama out exs soldiers!\n",
      "\n",
      "ARIO\n"
     ]
    }
   ],
   "source": [
    "print(sp.decode(model.generate(torch.ones((1,1), dtype=torch.long, device = device) * 80, 1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
