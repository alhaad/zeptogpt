{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Shakespeare Language Model\n",
    "\n",
    "GPT-like language model that is trained on the tinyshakespeare dataset. This notebook was written while following Karpathy's 'Let's build GPT' vide. The only notable difference is the use of SentencePiece tokenization instead of a character level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  2638k      0 --:--:-- --:--:-- --:--:-- 2643k\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from zeptogpt.model import SimpleGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tinyshakespeare') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/tinyshakespeare\n",
      "  input_format: \n",
      "  model_prefix: models/shakespeare_tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: \n",
      "\n",
      "  user_defined_symbols: \n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/tinyshakespeare\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108171\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563798\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33675 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25671\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25671 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12645 obj=11.7019 num_tokens=52717 num_tokens/piece=4.169\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10772 obj=9.46046 num_tokens=53065 num_tokens/piece=4.9262\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8077 obj=9.47102 num_tokens=56344 num_tokens/piece=6.97586\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8069 obj=9.43248 num_tokens=56395 num_tokens/piece=6.98909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6049 obj=9.62524 num_tokens=61716 num_tokens/piece=10.2027\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6049 obj=9.58053 num_tokens=61712 num_tokens/piece=10.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4536 obj=9.85482 num_tokens=68025 num_tokens/piece=14.9967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4536 obj=9.80345 num_tokens=68028 num_tokens/piece=14.9974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3402 obj=10.137 num_tokens=74970 num_tokens/piece=22.037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3402 obj=10.0857 num_tokens=74971 num_tokens/piece=22.0373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2551 obj=10.4554 num_tokens=81942 num_tokens/piece=32.1215\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2551 obj=10.4013 num_tokens=81951 num_tokens/piece=32.125\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1913 obj=10.8732 num_tokens=89431 num_tokens/piece=46.7491\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1913 obj=10.8064 num_tokens=89440 num_tokens/piece=46.7538\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1434 obj=11.3422 num_tokens=96817 num_tokens/piece=67.5153\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1434 obj=11.2617 num_tokens=96830 num_tokens/piece=67.5244\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=11.8572 num_tokens=103715 num_tokens/piece=94.2864\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=11.7728 num_tokens=103744 num_tokens/piece=94.3127\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/shakespeare_tokenizer_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/shakespeare_tokenizer_model.vocab\n",
      "trainer_interface.cc(707) LOG(WARNING) The piece [\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='data/tinyshakespeare',\n",
    "                               model_prefix='models/shakespeare_tokenizer_model',\n",
    "                               vocab_size=1000,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='unigram',\n",
    "                               remove_extra_whitespaces=False,\n",
    "                               user_defined_symbols=[\"\\n\", \"\\r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/shakespeare_tokenizer_model.model')\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  3, 175,  13,  66, 610,  26,  27, 200],\n",
       "         [ 97, 128,  10,   5,  77,  11,  46, 109],\n",
       "         [ 39,  16,  12, 709,  30,   3,   3, 191],\n",
       "         [101, 182,  20, 242,   5,  94, 388, 119]]),\n",
       " tensor([[175,  13,  66, 610,  26,  27, 200,  60],\n",
       "         [128,  10,   5,  77,  11,  46, 109, 130],\n",
       "         [ 16,  12, 709,  30,   3,   3, 191,  57],\n",
       "         [182,  20, 242,   5,  94, 388, 119,  36]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(sp.encode(text))\n",
    "\n",
    "traindata = data[:int(0.9 * len(data))]\n",
    "testdata = data[int(0.9 * len(data)):]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(data, device, batch_size, block_size):\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "get_batch(traindata, 'cpu', 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGPT(\n",
      "  (tok_emb_table): Embedding(1000, 32)\n",
      "  (pos_emb_table): Embedding(8, 32)\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): DecoderBlock(\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadedSelfAttention(\n",
      "        (c_attn): Linear(in_features=32, out_features=96, bias=True)\n",
      "        (c_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadedSelfAttention(\n",
      "        (c_attn): Linear(in_features=32, out_features=96, bias=True)\n",
      "        (c_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=32, out_features=1000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 21/10000 [00:00<02:40, 62.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=7.0566182136535645 Test Loss=7.075974941253662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1036/10000 [00:05<00:54, 165.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=5.057060718536377 Test Loss=4.96154260635376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2027/10000 [00:10<00:48, 165.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.6959309577941895 Test Loss=4.719968318939209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3044/10000 [00:15<00:38, 179.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.432583808898926 Test Loss=4.549083709716797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4041/10000 [00:20<00:40, 145.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.3404436111450195 Test Loss=4.444079399108887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5041/10000 [00:24<00:27, 177.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.250292778015137 Test Loss=4.368217468261719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6035/10000 [00:29<00:23, 172.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.156818389892578 Test Loss=4.302825927734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7028/10000 [00:33<00:17, 169.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.123233318328857 Test Loss=4.143256664276123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8032/10000 [00:39<00:21, 91.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.9391846656799316 Test Loss=4.1493916511535645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9034/10000 [00:44<00:05, 176.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.969677448272705 Test Loss=4.232929706573486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:48<00:00, 206.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.024553298950195 Test Loss=4.1811370849609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from tqdm import tqdm\n",
    "\n",
    "embed_dims = 32\n",
    "num_heads = 4\n",
    "num_decoder_layers = 2\n",
    "eval_iters = 100\n",
    "eval_interval = 1000\n",
    "num_training_iters = 10000\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SimpleGPT(vocab_size, embed_dims, block_size, num_heads, num_decoder_layers)\n",
    "model.to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "@torch.no_grad\n",
    "def estimate_loss(dataset):\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    model.eval()\n",
    "    for i in range(eval_iters):\n",
    "        inputs, targets = get_batch(dataset, device, batch_size, block_size)\n",
    "        logits = model(inputs)\n",
    "        B, T, C = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_training_iters)):\n",
    "    inputs, targets = get_batch(traindata, device, batch_size, block_size)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(inputs)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % eval_interval == 0 or i == num_training_iters - 1:\n",
    "        print(f\"Train Loss={estimate_loss(traindata)} Test Loss={estimate_loss(testdata)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sside?\n",
      "\n",
      "YSpetrusford,\n",
      "Upraa loy off, some toved Gey you a wifesusions the king to not stishness,\n",
      "Asd I lives,\n",
      "I am dorll been ladr daughter of klals\n",
      "THIth:\n",
      "Bey your badiererous underol\n",
      "And we hath, it: contheepmd. while; and blain of the go.\n",
      "\n",
      "SICINIUSur stet me.\n",
      "\n",
      "DUKEPJUNIOKE:\n",
      "I would hath not all darest then him with all being for feted to anil stros sqiceer with a fool,--eout fair prevenst deveg by forer joyds\n",
      "Hithgainst sweet greathood to us boy,\n",
      "The prurs by I I love me, and person to me, the love and thing's to opqutors to gimlying my conlddvergeor,\n",
      "Tell be stusting to come you were them made say 'tis see come,\n",
      "Thou will bet he,\n",
      "Wif of fool!\n",
      "\n",
      "JULIET:\n",
      "H:\n",
      "The day, heaven.\n",
      "\n",
      "Sat is the Lord me:\n",
      "\n",
      "Ind refose':\n",
      "As say, who is be mine,\n",
      "Lieldund.\n",
      "Where's boy\n",
      "TheLOUCESTE:\n",
      "Ane and chamed come:,\n",
      "And prod meidsic honestd,\n",
      "Spitness;\n",
      "Fors no is part, hence?\n",
      "\n",
      "Shoifb'er in ganer help: as I were it that of thy qulolsk king, the death.\n",
      "\n",
      "YUKEMIOAN:\n",
      "Who prestus;!\n",
      "Wich LAUur's her:\n",
      "As shall prison, Yet another thou firstbeathersts you, Bhenion and though!\n",
      "\n",
      "ALEONTage, traitor with a strengthencondry fitice and reubs\n",
      "Whed ever see's betteres with us's pardon a had.\n",
      "\n",
      "AGRELA:\n",
      "RIAy you well would boy. You not a greator sink of any young\n",
      "HORTH:\n",
      "\n",
      "Firal place not avesurst: you we there-- Edward his soldier\n",
      "w B soldiersaod.\n",
      "\n",
      "WARWICKRE:\n",
      "So-llacord, from but di;\n",
      "Whod are you: I can see your part. why not, thou had told been shalt five in the peace,\n",
      "And their daughter.\n",
      "\n",
      "Fortity, by I they thought is surfs'd offs life talk aotf manyrely, sweet them for the my lord;\n",
      "All after more inned propal fling goded theeentleman and to the\n",
      "Lron the kinged, 'tisgveed,\n",
      "Iship then to England!\n",
      "Sapreus that mbserl my did you gos you with Thou fie would I will: will the mied would Warwick of theirit old co\n",
      "Thoull showas's o's res him hear\n",
      "eter:\n",
      "ited, sir what hath by,; for in my dthrob of veptly's me, the Lancaster for his vsadbd with her dsighting your perents\n",
      "TDrid?RYice: may speak with come. Where all by them\n",
      "IVA:\n",
      "All, sir;\n",
      "ADY:\n",
      "A, was yet thou countarlation.\n",
      "\n",
      "I thought ever standurther, then ofBRath person to colyundi-peewe, or is out's on the this.\n",
      "\n",
      "TARIO my counsel breath's Clarence:\n",
      "Thounes silen crownst.\n",
      "\n",
      "BENVOLIO:\n",
      "Of,\n",
      "For ny thing to Lapiseks in but\n",
      "ick love to livey, and shech the grounds;\n",
      "qout Genitable a preble\n"
     ]
    }
   ],
   "source": [
    "print(sp.decode(model.generate(torch.ones((1,1), dtype=torch.long, device = device) * 80, 1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
