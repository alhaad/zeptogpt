{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Shakespeare Language Model\n",
    "\n",
    "GPT-like language model that is trained on the tinyshakespeare dataset. This notebook was written while following Karpathy's 'Let's build GPT' vide. The only notable difference is the use of SentencePiece tokenization instead of a character level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  2729k      0 --:--:-- --:--:-- --:--:-- 2736k\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from zeptogpt.model import SimpleGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tinyshakespeare') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/tinyshakespeare\n",
      "  input_format: \n",
      "  model_prefix: models/shakespeare_tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: \n",
      "\n",
      "  user_defined_symbols: \n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/tinyshakespeare\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108171\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563798\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33675 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25671\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25671 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12645 obj=11.7019 num_tokens=52717 num_tokens/piece=4.169\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10772 obj=9.46046 num_tokens=53065 num_tokens/piece=4.9262\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8077 obj=9.47102 num_tokens=56344 num_tokens/piece=6.97586\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8069 obj=9.43248 num_tokens=56395 num_tokens/piece=6.98909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6049 obj=9.62524 num_tokens=61716 num_tokens/piece=10.2027\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6049 obj=9.58053 num_tokens=61712 num_tokens/piece=10.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4536 obj=9.85482 num_tokens=68025 num_tokens/piece=14.9967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4536 obj=9.80345 num_tokens=68028 num_tokens/piece=14.9974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3402 obj=10.137 num_tokens=74970 num_tokens/piece=22.037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3402 obj=10.0857 num_tokens=74971 num_tokens/piece=22.0373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2551 obj=10.4554 num_tokens=81942 num_tokens/piece=32.1215\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2551 obj=10.4013 num_tokens=81951 num_tokens/piece=32.125\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1913 obj=10.8732 num_tokens=89431 num_tokens/piece=46.7491\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1913 obj=10.8064 num_tokens=89440 num_tokens/piece=46.7538\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1434 obj=11.3422 num_tokens=96817 num_tokens/piece=67.5153\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1434 obj=11.2617 num_tokens=96830 num_tokens/piece=67.5244\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=11.8572 num_tokens=103715 num_tokens/piece=94.2864\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=11.7728 num_tokens=103744 num_tokens/piece=94.3127\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/shakespeare_tokenizer_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/shakespeare_tokenizer_model.vocab\n",
      "trainer_interface.cc(707) LOG(WARNING) The piece [\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='data/tinyshakespeare',\n",
    "                               model_prefix='models/shakespeare_tokenizer_model',\n",
    "                               vocab_size=1000,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='unigram',\n",
    "                               remove_extra_whitespaces=False,\n",
    "                               user_defined_symbols=[\"\\n\", \"\\r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/shakespeare_tokenizer_model.model')\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  3, 175,  13,  66, 610,  26,  27, 200],\n",
       "         [ 97, 128,  10,   5,  77,  11,  46, 109],\n",
       "         [ 39,  16,  12, 709,  30,   3,   3, 191],\n",
       "         [101, 182,  20, 242,   5,  94, 388, 119]]),\n",
       " tensor([[175,  13,  66, 610,  26,  27, 200,  60],\n",
       "         [128,  10,   5,  77,  11,  46, 109, 130],\n",
       "         [ 16,  12, 709,  30,   3,   3, 191,  57],\n",
       "         [182,  20, 242,   5,  94, 388, 119,  36]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(sp.encode(text))\n",
    "\n",
    "traindata = data[:int(0.9 * len(data))]\n",
    "testdata = data[int(0.9 * len(data)):]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(data, device, batch_size, block_size):\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "get_batch(traindata, 'cpu', 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGPT(\n",
      "  (tok_emb_table): Embedding(1000, 32)\n",
      "  (pos_emb_table): Embedding(8, 32)\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): DecoderBlock(\n",
      "      (multi_headed_attn): MultiheadedSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x CausalSelfAttentionHead(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (multi_headed_attn): MultiheadedSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x CausalSelfAttentionHead(\n",
      "            (query): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (key): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/10000 [00:00<07:42, 21.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=7.220975875854492 Test Loss=7.2309088706970215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1015/10000 [00:10<02:09, 69.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.920136451721191 Test Loss=4.99624490737915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2014/10000 [00:19<01:54, 69.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.739692211151123 Test Loss=4.630186080932617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3013/10000 [00:29<01:40, 69.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.516595363616943 Test Loss=4.580789566040039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4023/10000 [00:38<01:26, 69.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.2872633934021 Test Loss=4.447488784790039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5021/10000 [00:48<01:14, 66.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.198359966278076 Test Loss=4.414018630981445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6017/10000 [00:58<00:59, 67.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.194443225860596 Test Loss=4.3003010749816895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7020/10000 [01:07<00:53, 55.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.151915073394775 Test Loss=4.285052299499512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8012/10000 [01:17<00:33, 59.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.117393493652344 Test Loss=4.273078918457031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9013/10000 [01:27<00:17, 55.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.110207557678223 Test Loss=4.218118190765381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:36<00:00, 103.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.9792938232421875 Test Loss=4.189849853515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from tqdm import tqdm\n",
    "\n",
    "embed_dims = 32\n",
    "num_heads = 4\n",
    "num_decoder_layers = 2\n",
    "eval_iters = 100\n",
    "eval_interval = 1000\n",
    "num_training_iters = 10000\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SimpleGPT(vocab_size, embed_dims, block_size, num_heads, num_decoder_layers)\n",
    "model.to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "@torch.no_grad\n",
    "def estimate_loss(dataset):\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    model.eval()\n",
    "    for i in range(eval_iters):\n",
    "        inputs, targets = get_batch(dataset, device, batch_size, block_size)\n",
    "        logits = model(inputs)\n",
    "        B, T, C = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_training_iters)):\n",
    "    inputs, targets = get_batch(traindata, device, batch_size, block_size)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(inputs)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % eval_interval == 0 or i == num_training_iters - 1:\n",
    "        print(f\"Train Loss={estimate_loss(traindata)} Test Loss={estimate_loss(testdata)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam and my false truermod to the brother,\n",
      "Thoidy,!d if, with a sssure, sir:\n",
      "Ly, thou why king?\n",
      "\n",
      "ROMEOLYSIO:\n",
      "Wt am you speation! commd itin and mour'\n",
      "dow hate's to you placeS:\n",
      "Hac?\n",
      "some thy sunfness.\n",
      "\n",
      "Toous as soor gone in my father means atr huned the wouldst hath your wri sigar good Englands\n",
      "This ter mad, where in the cish, brurare ase to sound I' the came then as ocd when him were it.\n",
      "\n",
      "PUCANIO:\n",
      "Firiery'd the woman than my Wesags interly night!\n",
      "\n",
      "Proviseten yet by the fityets by Geideforderd your consandivented,\n",
      "Fort than to goch with onew,\n",
      "Firthirs that.\n",
      "\n",
      "KING EDWARD this will, call what now? God nosed now;\n",
      "T Gy bend thus the tender wakeers,\n",
      "No temb is him sires\n",
      "Te come sidederd; I be to the stin\n",
      "Twes that not the disteving, where ceumionis,\n",
      "Dow have back, in good but power sounding, what some monasce'\n",
      "Ard we will your li presctt shall budclaolt the irstered.\n",
      "\n",
      "DIRSIANurson reter a inst there,\n",
      "Drpasory allONDss be sonreh, my day is whom os truth my seay me is the time an the partsmw coorders vollchk-y here not diLAUENNIUS:\n",
      "Coy, ta thent not Romeo, so soic'd I pray is show ones.\n",
      "\n",
      "DUCHKE:\n",
      "Yare,NCE zenuside,\n",
      "And by one promise a gentlemanst the bold sod till loill good spirit:\n",
      "Thake:\n",
      "For, and stand of my head?\n",
      "\n",
      "Wishn'ture but cityyer?\n",
      "\n",
      "Wo was is him sce,\n",
      "ingough their caughlis rer:\n",
      "Aner are the he the deecomicious trumenty, woe dind that with and have no huct you\n",
      "y, He birlas, as yourself and desckusykk seee city, should have not\n",
      "Tolu seemat cheer trtice to dar this bad,\n",
      "\n",
      "Tos locherod spirit Glike I do thou bos mantr up bubarly war with these sun-itsen to a chid the gchumber, than thee young no cuous must atly dear asr power myself:\n",
      "Swar'serityionty,\n",
      "Wiles we for might Powfumut roure my pity night:\n",
      "BeA:\n",
      "Novers crud:\n",
      "Wpingin flower, being away from the grief which be for thy a rengarilly upon her is gones be fe in my lords. But'dor the help pluck: who incpoo thy sighils a corterises him hand, be content welcome, to drn he pand, these move to not, hence, bes, and the state\n",
      "s prince better fear but a lord inages:\n",
      "Thall how difer for your loves and hate sir.\n",
      "\n",
      "QLEN ELIZABETH:\n",
      "Ciding swurthirt that course, a tmbrahay theely's most in fiss him cel to thy her will I hopeb, should.\n",
      "\n",
      "Thet is him cen,\n",
      "Thiroy mar baerbter Yet\n",
      "Ant as know as the varviloclolders.\n",
      "\n",
      "DORVAPKE O\n"
     ]
    }
   ],
   "source": [
    "print(sp.decode(model.generate(torch.ones((1,1), dtype=torch.long, device = device) * 80, 1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
