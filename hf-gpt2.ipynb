{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading GPT2 weights from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, I\\'m a language model, not a system.\" You know, something like this. I was playing with a set of concepts as a student'},\n",
       " {'generated_text': \"Hello, I'm a language model, not a code one. I can do any kind of machine algebra, though I'm not a programmatic language\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so if there is a question, you might take a closer look at it.\\n\\nThe main idea is\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, not a computer game.\"\\n\\nHis answer? \"I don\\'t know how many people were here that day'},\n",
       " {'generated_text': \"Hello, I'm a language model, a data model which works for every single function and function with three arguments.\\n\\nExample:\\n\\nval\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "sd_hf = model.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb_table.weight torch.Size([50257, 768])\n",
      "pos_emb_table.weight torch.Size([1024, 768])\n",
      "decoder_blocks.0.ln1.weight torch.Size([768])\n",
      "decoder_blocks.0.ln1.bias torch.Size([768])\n",
      "decoder_blocks.0.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.0.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.0.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.0.ln2.weight torch.Size([768])\n",
      "decoder_blocks.0.ln2.bias torch.Size([768])\n",
      "decoder_blocks.0.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.0.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.0.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.0.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.1.ln1.weight torch.Size([768])\n",
      "decoder_blocks.1.ln1.bias torch.Size([768])\n",
      "decoder_blocks.1.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.1.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.1.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.1.ln2.weight torch.Size([768])\n",
      "decoder_blocks.1.ln2.bias torch.Size([768])\n",
      "decoder_blocks.1.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.1.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.1.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.1.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.2.ln1.weight torch.Size([768])\n",
      "decoder_blocks.2.ln1.bias torch.Size([768])\n",
      "decoder_blocks.2.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.2.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.2.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.2.ln2.weight torch.Size([768])\n",
      "decoder_blocks.2.ln2.bias torch.Size([768])\n",
      "decoder_blocks.2.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.2.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.2.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.2.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.3.ln1.weight torch.Size([768])\n",
      "decoder_blocks.3.ln1.bias torch.Size([768])\n",
      "decoder_blocks.3.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.3.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.3.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.3.ln2.weight torch.Size([768])\n",
      "decoder_blocks.3.ln2.bias torch.Size([768])\n",
      "decoder_blocks.3.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.3.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.3.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.3.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.4.ln1.weight torch.Size([768])\n",
      "decoder_blocks.4.ln1.bias torch.Size([768])\n",
      "decoder_blocks.4.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.4.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.4.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.4.ln2.weight torch.Size([768])\n",
      "decoder_blocks.4.ln2.bias torch.Size([768])\n",
      "decoder_blocks.4.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.4.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.4.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.4.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.5.ln1.weight torch.Size([768])\n",
      "decoder_blocks.5.ln1.bias torch.Size([768])\n",
      "decoder_blocks.5.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.5.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.5.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.5.ln2.weight torch.Size([768])\n",
      "decoder_blocks.5.ln2.bias torch.Size([768])\n",
      "decoder_blocks.5.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.5.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.5.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.5.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.6.ln1.weight torch.Size([768])\n",
      "decoder_blocks.6.ln1.bias torch.Size([768])\n",
      "decoder_blocks.6.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.6.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.6.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.6.ln2.weight torch.Size([768])\n",
      "decoder_blocks.6.ln2.bias torch.Size([768])\n",
      "decoder_blocks.6.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.6.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.6.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.6.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.7.ln1.weight torch.Size([768])\n",
      "decoder_blocks.7.ln1.bias torch.Size([768])\n",
      "decoder_blocks.7.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.7.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.7.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.7.ln2.weight torch.Size([768])\n",
      "decoder_blocks.7.ln2.bias torch.Size([768])\n",
      "decoder_blocks.7.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.7.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.7.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.7.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.8.ln1.weight torch.Size([768])\n",
      "decoder_blocks.8.ln1.bias torch.Size([768])\n",
      "decoder_blocks.8.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.8.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.8.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.8.ln2.weight torch.Size([768])\n",
      "decoder_blocks.8.ln2.bias torch.Size([768])\n",
      "decoder_blocks.8.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.8.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.8.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.8.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.9.ln1.weight torch.Size([768])\n",
      "decoder_blocks.9.ln1.bias torch.Size([768])\n",
      "decoder_blocks.9.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.9.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.9.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.9.ln2.weight torch.Size([768])\n",
      "decoder_blocks.9.ln2.bias torch.Size([768])\n",
      "decoder_blocks.9.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.9.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.9.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.9.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.10.ln1.weight torch.Size([768])\n",
      "decoder_blocks.10.ln1.bias torch.Size([768])\n",
      "decoder_blocks.10.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.10.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.10.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.10.ln2.weight torch.Size([768])\n",
      "decoder_blocks.10.ln2.bias torch.Size([768])\n",
      "decoder_blocks.10.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.10.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.10.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.10.mlp.2.bias torch.Size([768])\n",
      "decoder_blocks.11.ln1.weight torch.Size([768])\n",
      "decoder_blocks.11.ln1.bias torch.Size([768])\n",
      "decoder_blocks.11.attn.c_attn.weight torch.Size([2304, 768])\n",
      "decoder_blocks.11.attn.c_attn.bias torch.Size([2304])\n",
      "decoder_blocks.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder_blocks.11.attn.c_proj.bias torch.Size([768])\n",
      "decoder_blocks.11.ln2.weight torch.Size([768])\n",
      "decoder_blocks.11.ln2.bias torch.Size([768])\n",
      "decoder_blocks.11.mlp.0.weight torch.Size([3072, 768])\n",
      "decoder_blocks.11.mlp.0.bias torch.Size([3072])\n",
      "decoder_blocks.11.mlp.2.weight torch.Size([768, 3072])\n",
      "decoder_blocks.11.mlp.2.bias torch.Size([768])\n",
      "ln_f.weight torch.Size([768])\n",
      "ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "import zeptogpt\n",
    "import importlib\n",
    "importlib.reload(zeptogpt)\n",
    "\n",
    "from zeptogpt.model import SimpleGPT\n",
    "\n",
    "vocab_size = 50257\n",
    "embed_dim = 768\n",
    "block_size = 1024\n",
    "num_heads = 12\n",
    "num_decoder_layers = 12\n",
    "GPT2 = SimpleGPT(vocab_size, embed_dim, block_size, num_heads, num_decoder_layers)\n",
    "\n",
    "sd = GPT2.state_dict()\n",
    "for k, v in sd.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: transformer.wte.weight -> tok_emb_table.weight\n",
      "Copied: transformer.wpe.weight -> pos_emb_table.weight\n",
      "Copied: transformer.h.0.ln_1.weight -> decoder_blocks.0.ln1.weight\n",
      "Copied: transformer.h.0.ln_1.bias -> decoder_blocks.0.ln1.bias\n",
      "Copied with transpose: transformer.h.0.attn.c_attn.weight -> decoder_blocks.0.attn.c_attn.weight\n",
      "Copied: transformer.h.0.attn.c_attn.bias -> decoder_blocks.0.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.0.attn.c_proj.weight -> decoder_blocks.0.attn.c_proj.weight\n",
      "Copied: transformer.h.0.attn.c_proj.bias -> decoder_blocks.0.attn.c_proj.bias\n",
      "Copied: transformer.h.0.ln_2.weight -> decoder_blocks.0.ln2.weight\n",
      "Copied: transformer.h.0.ln_2.bias -> decoder_blocks.0.ln2.bias\n",
      "Copied with transpose: transformer.h.0.mlp.c_fc.weight -> decoder_blocks.0.mlp.0.weight\n",
      "Copied: transformer.h.0.mlp.c_fc.bias -> decoder_blocks.0.mlp.0.bias\n",
      "Copied with transpose: transformer.h.0.mlp.c_proj.weight -> decoder_blocks.0.mlp.2.weight\n",
      "Copied: transformer.h.0.mlp.c_proj.bias -> decoder_blocks.0.mlp.2.bias\n",
      "Copied: transformer.h.1.ln_1.weight -> decoder_blocks.1.ln1.weight\n",
      "Copied: transformer.h.1.ln_1.bias -> decoder_blocks.1.ln1.bias\n",
      "Copied with transpose: transformer.h.1.attn.c_attn.weight -> decoder_blocks.1.attn.c_attn.weight\n",
      "Copied: transformer.h.1.attn.c_attn.bias -> decoder_blocks.1.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.1.attn.c_proj.weight -> decoder_blocks.1.attn.c_proj.weight\n",
      "Copied: transformer.h.1.attn.c_proj.bias -> decoder_blocks.1.attn.c_proj.bias\n",
      "Copied: transformer.h.1.ln_2.weight -> decoder_blocks.1.ln2.weight\n",
      "Copied: transformer.h.1.ln_2.bias -> decoder_blocks.1.ln2.bias\n",
      "Copied with transpose: transformer.h.1.mlp.c_fc.weight -> decoder_blocks.1.mlp.0.weight\n",
      "Copied: transformer.h.1.mlp.c_fc.bias -> decoder_blocks.1.mlp.0.bias\n",
      "Copied with transpose: transformer.h.1.mlp.c_proj.weight -> decoder_blocks.1.mlp.2.weight\n",
      "Copied: transformer.h.1.mlp.c_proj.bias -> decoder_blocks.1.mlp.2.bias\n",
      "Copied: transformer.h.2.ln_1.weight -> decoder_blocks.2.ln1.weight\n",
      "Copied: transformer.h.2.ln_1.bias -> decoder_blocks.2.ln1.bias\n",
      "Copied with transpose: transformer.h.2.attn.c_attn.weight -> decoder_blocks.2.attn.c_attn.weight\n",
      "Copied: transformer.h.2.attn.c_attn.bias -> decoder_blocks.2.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.2.attn.c_proj.weight -> decoder_blocks.2.attn.c_proj.weight\n",
      "Copied: transformer.h.2.attn.c_proj.bias -> decoder_blocks.2.attn.c_proj.bias\n",
      "Copied: transformer.h.2.ln_2.weight -> decoder_blocks.2.ln2.weight\n",
      "Copied: transformer.h.2.ln_2.bias -> decoder_blocks.2.ln2.bias\n",
      "Copied with transpose: transformer.h.2.mlp.c_fc.weight -> decoder_blocks.2.mlp.0.weight\n",
      "Copied: transformer.h.2.mlp.c_fc.bias -> decoder_blocks.2.mlp.0.bias\n",
      "Copied with transpose: transformer.h.2.mlp.c_proj.weight -> decoder_blocks.2.mlp.2.weight\n",
      "Copied: transformer.h.2.mlp.c_proj.bias -> decoder_blocks.2.mlp.2.bias\n",
      "Copied: transformer.h.3.ln_1.weight -> decoder_blocks.3.ln1.weight\n",
      "Copied: transformer.h.3.ln_1.bias -> decoder_blocks.3.ln1.bias\n",
      "Copied with transpose: transformer.h.3.attn.c_attn.weight -> decoder_blocks.3.attn.c_attn.weight\n",
      "Copied: transformer.h.3.attn.c_attn.bias -> decoder_blocks.3.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.3.attn.c_proj.weight -> decoder_blocks.3.attn.c_proj.weight\n",
      "Copied: transformer.h.3.attn.c_proj.bias -> decoder_blocks.3.attn.c_proj.bias\n",
      "Copied: transformer.h.3.ln_2.weight -> decoder_blocks.3.ln2.weight\n",
      "Copied: transformer.h.3.ln_2.bias -> decoder_blocks.3.ln2.bias\n",
      "Copied with transpose: transformer.h.3.mlp.c_fc.weight -> decoder_blocks.3.mlp.0.weight\n",
      "Copied: transformer.h.3.mlp.c_fc.bias -> decoder_blocks.3.mlp.0.bias\n",
      "Copied with transpose: transformer.h.3.mlp.c_proj.weight -> decoder_blocks.3.mlp.2.weight\n",
      "Copied: transformer.h.3.mlp.c_proj.bias -> decoder_blocks.3.mlp.2.bias\n",
      "Copied: transformer.h.4.ln_1.weight -> decoder_blocks.4.ln1.weight\n",
      "Copied: transformer.h.4.ln_1.bias -> decoder_blocks.4.ln1.bias\n",
      "Copied with transpose: transformer.h.4.attn.c_attn.weight -> decoder_blocks.4.attn.c_attn.weight\n",
      "Copied: transformer.h.4.attn.c_attn.bias -> decoder_blocks.4.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.4.attn.c_proj.weight -> decoder_blocks.4.attn.c_proj.weight\n",
      "Copied: transformer.h.4.attn.c_proj.bias -> decoder_blocks.4.attn.c_proj.bias\n",
      "Copied: transformer.h.4.ln_2.weight -> decoder_blocks.4.ln2.weight\n",
      "Copied: transformer.h.4.ln_2.bias -> decoder_blocks.4.ln2.bias\n",
      "Copied with transpose: transformer.h.4.mlp.c_fc.weight -> decoder_blocks.4.mlp.0.weight\n",
      "Copied: transformer.h.4.mlp.c_fc.bias -> decoder_blocks.4.mlp.0.bias\n",
      "Copied with transpose: transformer.h.4.mlp.c_proj.weight -> decoder_blocks.4.mlp.2.weight\n",
      "Copied: transformer.h.4.mlp.c_proj.bias -> decoder_blocks.4.mlp.2.bias\n",
      "Copied: transformer.h.5.ln_1.weight -> decoder_blocks.5.ln1.weight\n",
      "Copied: transformer.h.5.ln_1.bias -> decoder_blocks.5.ln1.bias\n",
      "Copied with transpose: transformer.h.5.attn.c_attn.weight -> decoder_blocks.5.attn.c_attn.weight\n",
      "Copied: transformer.h.5.attn.c_attn.bias -> decoder_blocks.5.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.5.attn.c_proj.weight -> decoder_blocks.5.attn.c_proj.weight\n",
      "Copied: transformer.h.5.attn.c_proj.bias -> decoder_blocks.5.attn.c_proj.bias\n",
      "Copied: transformer.h.5.ln_2.weight -> decoder_blocks.5.ln2.weight\n",
      "Copied: transformer.h.5.ln_2.bias -> decoder_blocks.5.ln2.bias\n",
      "Copied with transpose: transformer.h.5.mlp.c_fc.weight -> decoder_blocks.5.mlp.0.weight\n",
      "Copied: transformer.h.5.mlp.c_fc.bias -> decoder_blocks.5.mlp.0.bias\n",
      "Copied with transpose: transformer.h.5.mlp.c_proj.weight -> decoder_blocks.5.mlp.2.weight\n",
      "Copied: transformer.h.5.mlp.c_proj.bias -> decoder_blocks.5.mlp.2.bias\n",
      "Copied: transformer.h.6.ln_1.weight -> decoder_blocks.6.ln1.weight\n",
      "Copied: transformer.h.6.ln_1.bias -> decoder_blocks.6.ln1.bias\n",
      "Copied with transpose: transformer.h.6.attn.c_attn.weight -> decoder_blocks.6.attn.c_attn.weight\n",
      "Copied: transformer.h.6.attn.c_attn.bias -> decoder_blocks.6.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.6.attn.c_proj.weight -> decoder_blocks.6.attn.c_proj.weight\n",
      "Copied: transformer.h.6.attn.c_proj.bias -> decoder_blocks.6.attn.c_proj.bias\n",
      "Copied: transformer.h.6.ln_2.weight -> decoder_blocks.6.ln2.weight\n",
      "Copied: transformer.h.6.ln_2.bias -> decoder_blocks.6.ln2.bias\n",
      "Copied with transpose: transformer.h.6.mlp.c_fc.weight -> decoder_blocks.6.mlp.0.weight\n",
      "Copied: transformer.h.6.mlp.c_fc.bias -> decoder_blocks.6.mlp.0.bias\n",
      "Copied with transpose: transformer.h.6.mlp.c_proj.weight -> decoder_blocks.6.mlp.2.weight\n",
      "Copied: transformer.h.6.mlp.c_proj.bias -> decoder_blocks.6.mlp.2.bias\n",
      "Copied: transformer.h.7.ln_1.weight -> decoder_blocks.7.ln1.weight\n",
      "Copied: transformer.h.7.ln_1.bias -> decoder_blocks.7.ln1.bias\n",
      "Copied with transpose: transformer.h.7.attn.c_attn.weight -> decoder_blocks.7.attn.c_attn.weight\n",
      "Copied: transformer.h.7.attn.c_attn.bias -> decoder_blocks.7.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.7.attn.c_proj.weight -> decoder_blocks.7.attn.c_proj.weight\n",
      "Copied: transformer.h.7.attn.c_proj.bias -> decoder_blocks.7.attn.c_proj.bias\n",
      "Copied: transformer.h.7.ln_2.weight -> decoder_blocks.7.ln2.weight\n",
      "Copied: transformer.h.7.ln_2.bias -> decoder_blocks.7.ln2.bias\n",
      "Copied with transpose: transformer.h.7.mlp.c_fc.weight -> decoder_blocks.7.mlp.0.weight\n",
      "Copied: transformer.h.7.mlp.c_fc.bias -> decoder_blocks.7.mlp.0.bias\n",
      "Copied with transpose: transformer.h.7.mlp.c_proj.weight -> decoder_blocks.7.mlp.2.weight\n",
      "Copied: transformer.h.7.mlp.c_proj.bias -> decoder_blocks.7.mlp.2.bias\n",
      "Copied: transformer.h.8.ln_1.weight -> decoder_blocks.8.ln1.weight\n",
      "Copied: transformer.h.8.ln_1.bias -> decoder_blocks.8.ln1.bias\n",
      "Copied with transpose: transformer.h.8.attn.c_attn.weight -> decoder_blocks.8.attn.c_attn.weight\n",
      "Copied: transformer.h.8.attn.c_attn.bias -> decoder_blocks.8.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.8.attn.c_proj.weight -> decoder_blocks.8.attn.c_proj.weight\n",
      "Copied: transformer.h.8.attn.c_proj.bias -> decoder_blocks.8.attn.c_proj.bias\n",
      "Copied: transformer.h.8.ln_2.weight -> decoder_blocks.8.ln2.weight\n",
      "Copied: transformer.h.8.ln_2.bias -> decoder_blocks.8.ln2.bias\n",
      "Copied with transpose: transformer.h.8.mlp.c_fc.weight -> decoder_blocks.8.mlp.0.weight\n",
      "Copied: transformer.h.8.mlp.c_fc.bias -> decoder_blocks.8.mlp.0.bias\n",
      "Copied with transpose: transformer.h.8.mlp.c_proj.weight -> decoder_blocks.8.mlp.2.weight\n",
      "Copied: transformer.h.8.mlp.c_proj.bias -> decoder_blocks.8.mlp.2.bias\n",
      "Copied: transformer.h.9.ln_1.weight -> decoder_blocks.9.ln1.weight\n",
      "Copied: transformer.h.9.ln_1.bias -> decoder_blocks.9.ln1.bias\n",
      "Copied with transpose: transformer.h.9.attn.c_attn.weight -> decoder_blocks.9.attn.c_attn.weight\n",
      "Copied: transformer.h.9.attn.c_attn.bias -> decoder_blocks.9.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.9.attn.c_proj.weight -> decoder_blocks.9.attn.c_proj.weight\n",
      "Copied: transformer.h.9.attn.c_proj.bias -> decoder_blocks.9.attn.c_proj.bias\n",
      "Copied: transformer.h.9.ln_2.weight -> decoder_blocks.9.ln2.weight\n",
      "Copied: transformer.h.9.ln_2.bias -> decoder_blocks.9.ln2.bias\n",
      "Copied with transpose: transformer.h.9.mlp.c_fc.weight -> decoder_blocks.9.mlp.0.weight\n",
      "Copied: transformer.h.9.mlp.c_fc.bias -> decoder_blocks.9.mlp.0.bias\n",
      "Copied with transpose: transformer.h.9.mlp.c_proj.weight -> decoder_blocks.9.mlp.2.weight\n",
      "Copied: transformer.h.9.mlp.c_proj.bias -> decoder_blocks.9.mlp.2.bias\n",
      "Copied: transformer.h.10.ln_1.weight -> decoder_blocks.10.ln1.weight\n",
      "Copied: transformer.h.10.ln_1.bias -> decoder_blocks.10.ln1.bias\n",
      "Copied with transpose: transformer.h.10.attn.c_attn.weight -> decoder_blocks.10.attn.c_attn.weight\n",
      "Copied: transformer.h.10.attn.c_attn.bias -> decoder_blocks.10.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.10.attn.c_proj.weight -> decoder_blocks.10.attn.c_proj.weight\n",
      "Copied: transformer.h.10.attn.c_proj.bias -> decoder_blocks.10.attn.c_proj.bias\n",
      "Copied: transformer.h.10.ln_2.weight -> decoder_blocks.10.ln2.weight\n",
      "Copied: transformer.h.10.ln_2.bias -> decoder_blocks.10.ln2.bias\n",
      "Copied with transpose: transformer.h.10.mlp.c_fc.weight -> decoder_blocks.10.mlp.0.weight\n",
      "Copied: transformer.h.10.mlp.c_fc.bias -> decoder_blocks.10.mlp.0.bias\n",
      "Copied with transpose: transformer.h.10.mlp.c_proj.weight -> decoder_blocks.10.mlp.2.weight\n",
      "Copied: transformer.h.10.mlp.c_proj.bias -> decoder_blocks.10.mlp.2.bias\n",
      "Copied: transformer.h.11.ln_1.weight -> decoder_blocks.11.ln1.weight\n",
      "Copied: transformer.h.11.ln_1.bias -> decoder_blocks.11.ln1.bias\n",
      "Copied with transpose: transformer.h.11.attn.c_attn.weight -> decoder_blocks.11.attn.c_attn.weight\n",
      "Copied: transformer.h.11.attn.c_attn.bias -> decoder_blocks.11.attn.c_attn.bias\n",
      "Copied with transpose: transformer.h.11.attn.c_proj.weight -> decoder_blocks.11.attn.c_proj.weight\n",
      "Copied: transformer.h.11.attn.c_proj.bias -> decoder_blocks.11.attn.c_proj.bias\n",
      "Copied: transformer.h.11.ln_2.weight -> decoder_blocks.11.ln2.weight\n",
      "Copied: transformer.h.11.ln_2.bias -> decoder_blocks.11.ln2.bias\n",
      "Copied with transpose: transformer.h.11.mlp.c_fc.weight -> decoder_blocks.11.mlp.0.weight\n",
      "Copied: transformer.h.11.mlp.c_fc.bias -> decoder_blocks.11.mlp.0.bias\n",
      "Copied with transpose: transformer.h.11.mlp.c_proj.weight -> decoder_blocks.11.mlp.2.weight\n",
      "Copied: transformer.h.11.mlp.c_proj.bias -> decoder_blocks.11.mlp.2.bias\n",
      "Copied: transformer.ln_f.weight -> ln_f.weight\n",
      "Copied: transformer.ln_f.bias -> ln_f.bias\n",
      "Copied: lm_head.weight -> lm_head.weight\n",
      "Weight copying process completed!\n"
     ]
    }
   ],
   "source": [
    "def copy_weights(model1, model2):\n",
    "    layer_mapping = {\n",
    "        'transformer.wte.weight': 'tok_emb_table.weight',\n",
    "        'transformer.wpe.weight': 'pos_emb_table.weight',\n",
    "        'transformer.h.': 'decoder_blocks.',\n",
    "        'ln_1': 'ln1',\n",
    "        'ln_2': 'ln2',\n",
    "        'attn.c_attn': 'attn.c_attn',\n",
    "        'attn.c_proj': 'attn.c_proj',\n",
    "        'mlp.c_fc': 'mlp.0',\n",
    "        'mlp.c_proj': 'mlp.2',\n",
    "        'transformer.ln_f': 'ln_f',\n",
    "        'lm_head.weight': 'lm_head.weight'\n",
    "    }\n",
    "\n",
    "    # Whitelist of parameters to be transposed\n",
    "    transpose_whitelist = [\n",
    "        'attn.c_proj.weight',\n",
    "        'attn.c_attn.weight',\n",
    "        'mlp.c_fc.weight',\n",
    "        'mlp.c_proj.weight'\n",
    "    ]\n",
    "\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "\n",
    "    for name1, param1 in state_dict1.items():\n",
    "        name2 = name1\n",
    "        for old, new in layer_mapping.items():\n",
    "            if old in name2:\n",
    "                name2 = name2.replace(old, new)\n",
    "        \n",
    "        if 'transformer.h.' in name1:\n",
    "            block_num = name1.split('.')[2]\n",
    "            name2 = name2.replace(f'transformer.h.{block_num}', f'decoder_blocks.{block_num}')\n",
    "\n",
    "        if name2 in state_dict2:\n",
    "            param2 = state_dict2[name2]\n",
    "            should_transpose = any(t in name2 for t in transpose_whitelist)\n",
    "\n",
    "            if param1.shape == param2.shape and not should_transpose:\n",
    "                param2.copy_(param1)\n",
    "                print(f\"Copied: {name1} -> {name2}\")\n",
    "            elif param1.shape == param2.shape[::-1] or should_transpose:\n",
    "                param2.copy_(param1.t())\n",
    "                print(f\"Copied with transpose: {name1} -> {name2}\")\n",
    "            else:\n",
    "                print(f\"Shape mismatch for {name1} -> {name2}: {param1.shape} vs {param2.shape}\")\n",
    "        else:\n",
    "            print(f\"No matching parameter found for {name1} in model2\")\n",
    "\n",
    "    model2.load_state_dict(state_dict2)\n",
    "    print(\"Weight copying process completed!\")\n",
    "\n",
    "# Usage:\n",
    "copy_weights(model, GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, which was decided into different. Let me tell us. Rarely, these borders brought to Hollywood's-they looked just, and were even.After\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "input = enc.encode(\"Hello, I'm a language model,\")\n",
    "output = GPT2.generate(torch.tensor([input]), 30)\n",
    "print(enc.decode(output[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
