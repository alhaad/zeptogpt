{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GPT for Modular Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from zeptogpt.model import GPT\n",
    "from zeptogpt.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<5+4+8=7>\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary, encoder and decoder.\n",
    "\n",
    "# Our modular arithmetic is in a world with only 0....MOD numbers\n",
    "MOD = 10\n",
    "\n",
    "stoi = {}\n",
    "for i in range(MOD):\n",
    "    stoi[str(i)] = i\n",
    "\n",
    "# The mathematical operators we want to support\n",
    "stoi['+'] = MOD\n",
    "\n",
    "stoi['='] = MOD+1\n",
    "\n",
    "# Special tokens\n",
    "stoi['<'] = MOD+2\n",
    "stoi['>'] = MOD+3\n",
    "\n",
    "# Padding\n",
    "stoi['.'] = MOD+4\n",
    "\n",
    "vocab = list(stoi.keys())\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "encode = lambda x: [stoi[s] for s in x]\n",
    "\n",
    "itos = {v:k for k, v in stoi.items()}\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(decode(encode('<5+4+8=7>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<6+5=1>...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_expr(block_size):\n",
    "    num_terms = random.randint(1, block_size // 2 - 2)\n",
    "    numbers = [random.randint(0, MOD-1) for _ in range(num_terms)]\n",
    "    total = sum(numbers) % MOD\n",
    "    expr = '<' + '+'.join(map(str, numbers)) + f'={total}' + '>'\n",
    "    expr = expr + '.' * (block_size - len(expr))\n",
    "    return expr\n",
    "\n",
    "print(generate_expr(10))\n",
    "\n",
    "block_size=16\n",
    "inputs_expr = [generate_expr(block_size) for _ in range(10000)]\n",
    "def prepare_target(expr):\n",
    "    expr = expr[1:] + '.'\n",
    "    equal_pos = expr.index('=')\n",
    "    expr = '.' * (equal_pos + 1) + expr[equal_pos + 1:]\n",
    "    return expr\n",
    "targets_expr = list(map(prepare_target, inputs_expr))\n",
    "\n",
    "inputs = jnp.array(list(map(lambda x: jnp.array(encode(x)), inputs_expr)))\n",
    "targets = jnp.array(list(map(lambda x: jnp.array(encode(x)), targets_expr)))\n",
    "data = (inputs, targets)\n",
    "traindata = (inputs[:int(0.9 * len(inputs))], targets[:int(0.9 * len(targets))])\n",
    "testdata = (inputs[int(0.9 * len(inputs)):], targets[int(0.9 * len(targets)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([[12,  7, 10,  2, 11,  9, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14],\n",
      "       [12,  6, 10,  9, 10,  8, 11,  3, 13, 14, 14, 14, 14, 14, 14, 14],\n",
      "       [12,  5, 10,  7, 10,  1, 10,  2, 10,  8, 10,  2, 11,  5, 13, 14],\n",
      "       [12,  6, 10,  7, 10,  6, 10,  3, 10,  3, 10,  4, 11,  9, 13, 14]],      dtype=int32), Array([[14, 14, 14, 14,  9, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],\n",
      "       [14, 14, 14, 14, 14, 14,  3, 13, 14, 14, 14, 14, 14, 14, 14, 14],\n",
      "       [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,  5, 13, 14, 14],\n",
      "       [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,  9, 13, 14, 14]],      dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['batch_size'])\n",
    "def get_batch(key, data, batch_size):\n",
    "    inputs, targets = data\n",
    "    ix = jax.random.randint(key, shape=(), minval=0, maxval=(inputs.shape[0] // batch_size - 1))\n",
    "    x = jax.lax.dynamic_slice(inputs, (ix, 0), (batch_size, block_size))\n",
    "    y = jax.lax.dynamic_slice(targets, (ix, 0), (batch_size, block_size))\n",
    "    return x, y\n",
    "\n",
    "key = jax.random.key(1337)\n",
    "for _ in range(1):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    print(get_batch(key, testdata, 4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/10000 [00:08<1:27:38,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=2.991473913192749 Test Loss=2.904721736907959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1023/10000 [00:16<01:15, 118.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.2542179226875305 Test Loss=2.0593838691711426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2020/10000 [00:25<01:07, 118.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.005840673111379147 Test Loss=3.2911603450775146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3021/10000 [00:33<01:00, 114.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.0018675432074815035 Test Loss=4.103436470031738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4022/10000 [00:42<00:52, 113.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.000592435768339783 Test Loss=2.883065938949585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5015/10000 [00:51<00:42, 118.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.0003536375588737428 Test Loss=4.593021392822266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6022/10000 [01:00<00:33, 117.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.00018250728317070752 Test Loss=4.157101631164551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7023/10000 [01:08<00:25, 114.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.00010683573782444 Test Loss=3.236886978149414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8016/10000 [01:17<00:17, 113.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=6.839857815066352e-05 Test Loss=5.672919750213623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9022/10000 [01:26<00:08, 110.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.690494122565724e-05 Test Loss=4.671977996826172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:34<00:00, 105.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.7244043192476965e-05 Test Loss=4.853103160858154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "import functools\n",
    "\n",
    "# Hyperparamters\n",
    "num_iterations = 10000\n",
    "eval_interval = 1000\n",
    "eval_iters = 1\n",
    "block_size = block_size\n",
    "batch_size = 16\n",
    "embed_dim = 16\n",
    "num_heads = 8\n",
    "num_decoder_layers = 4\n",
    "\n",
    "key = jax.random.key(137)\n",
    "model = GPT(vocab_size, block_size, embed_dim, num_heads, num_decoder_layers)\n",
    "params = model.init(key, jnp.ones((1, block_size), dtype=jnp.int32))\n",
    "optimizer = optax.adamw(learning_rate=0.002)\n",
    "\n",
    "# Create training state\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "def cross_entropy_loss_with_ignore_index(logits, targets, ignore_index):\n",
    "    # Create a mask for valid (non-ignored) targets\n",
    "    mask = jnp.not_equal(targets, ignore_index)\n",
    "    # Compute log softmax of logits\n",
    "    log_softmax = jax.nn.log_softmax(logits, axis=-1)\n",
    "    # Compute the cross-entropy loss for all targets\n",
    "    targets_one_hot = jax.nn.one_hot(targets, logits.shape[-1])\n",
    "    loss = -jnp.sum(log_softmax * targets_one_hot, axis=-1)\n",
    "    # Apply mask to the loss\n",
    "    masked_loss = loss * mask\n",
    "    # Compute mean over non-ignored elements\n",
    "    return jnp.sum(masked_loss) / jnp.maximum(jnp.sum(mask), 1)\n",
    "ignore_index = stoi['.']  # Assuming stoi is your string-to-index mapping\n",
    "loss_fn = lambda logits, targets: cross_entropy_loss_with_ignore_index(logits, targets, ignore_index)\n",
    "train_batch_fn = functools.partial(get_batch, data=traindata, batch_size=batch_size)\n",
    "test_batch_fn = functools.partial(get_batch, data=testdata, batch_size=batch_size)\n",
    "trainer = Trainer(state, loss_fn, train_batch_fn, test_batch_fn, eval_iters, eval_interval, num_iterations)\n",
    "state = trainer.train(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<1+8+1=8>\n"
     ]
    }
   ],
   "source": [
    "def generate(model, params, input):\n",
    "    input = jnp.array([encode(input)])\n",
    "    for _ in range(block_size):\n",
    "        logits = model.apply(params, input)[:,-1,:]\n",
    "        new_token = jax.random.categorical(key, logits, shape=(1,1))\n",
    "        input = jnp.concatenate([input, new_token], axis=1)\n",
    "        if new_token[0] == stoi['>']:\n",
    "            break\n",
    "    return decode(input[0].tolist())\n",
    "\n",
    "\n",
    "print(generate(model, state.params, \"<1+9=\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
