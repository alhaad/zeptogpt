{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GPT for Modular Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 832, 28, 325, 135, 385, 248, 357, 981, 192, 939]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from zeptogpt.model import SimpleGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<5+4+8=7>\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary, encoder and decoder.\n",
    "\n",
    "# Our modular arithmetic is in a world with only 0....MOD numbers\n",
    "MOD = 10\n",
    "\n",
    "stoi = {}\n",
    "for i in range(MOD):\n",
    "    stoi[str(i)] = i\n",
    "\n",
    "# The mathematical operators we want to support\n",
    "stoi['+'] = MOD\n",
    "\n",
    "stoi['='] = MOD+1\n",
    "\n",
    "# Special tokens\n",
    "stoi['<'] = MOD+2\n",
    "stoi['>'] = MOD+3\n",
    "\n",
    "# Padding\n",
    "stoi['.'] = MOD+4\n",
    "\n",
    "vocab = list(stoi.keys())\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "encode = lambda x: [stoi[s] for s in x]\n",
    "\n",
    "itos = {v:k for k, v in stoi.items()}\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(decode(encode('<5+4+8=7>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12,  7, 10,  4, 10,  8, 10,  0, 10,  8, 11,  7, 13, 14, 14, 14],\n",
      "        [12,  9, 10,  7, 10,  5, 10,  4, 11,  5, 13, 14, 14, 14, 14, 14],\n",
      "        [12,  1, 10,  0, 10,  3, 10,  1, 10,  1, 10,  0, 11,  6, 13, 14],\n",
      "        [12,  2, 10,  7, 10,  9, 10,  6, 10,  3, 10,  4, 11,  1, 13, 14]])\n",
      "tensor([[14, 14, 14, 14, 14, 14, 14, 14, 14, 14,  7, 13, 14, 14, 14, 14],\n",
      "        [14, 14, 14, 14, 14, 14, 14, 14,  5, 13, 14, 14, 14, 14, 14, 14],\n",
      "        [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,  6, 13, 14, 14],\n",
      "        [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,  1, 13, 14, 14]])\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import random\n",
    "\n",
    "class ModularArithmeticDataset(IterableDataset):\n",
    "    def __init__(self, block_size, validation_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.skip_exprs = [self.generate_expr() for _ in range(validation_size)]\n",
    "        self.is_validation_mode = False\n",
    "    \n",
    "    def validation_mode(self):\n",
    "        self.is_validation_mode = True\n",
    "    \n",
    "    def training_mode(self):\n",
    "        self.is_validation_mode = False\n",
    "    \n",
    "    def generate_expr(self):\n",
    "        num_terms = random.randint(1, self.block_size // 2 - 2)\n",
    "        numbers = [random.randint(0, MOD-1) for _ in range(num_terms)]\n",
    "        total = sum(numbers) % MOD\n",
    "        expr = '<' + '+'.join(map(str, numbers)) + f'={total}' + '>'\n",
    "        expr = expr + '.' * (self.block_size - len(expr))\n",
    "        return expr\n",
    "    \n",
    "    def prepare_input(self, expr):\n",
    "        return torch.tensor(encode(expr))\n",
    "\n",
    "    def prepare_target(self, expr):\n",
    "        expr = expr[1:] + '.'\n",
    "        equal_pos = expr.index('=')\n",
    "        expr = '.' * (equal_pos + 1) + expr[equal_pos + 1:]\n",
    "        return torch.tensor(encode(expr))\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.is_validation_mode:\n",
    "            for skip_expr in self.skip_exprs:\n",
    "                yield self.prepare_input(skip_expr), self.prepare_target(skip_expr)\n",
    "            return\n",
    "        while True:\n",
    "            expr = self.generate_expr()\n",
    "            if expr in self.skip_exprs:\n",
    "                continue\n",
    "            yield self.prepare_input(expr), self.prepare_target(expr)\n",
    "dataset = ModularArithmeticDataset(16, 0)\n",
    "dataloader = DataLoader(dataset, 4)\n",
    "for ix, (input, target) in enumerate(dataloader):\n",
    "    print(input)\n",
    "    print(target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/10000 [00:01<36:27,  4.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.7799081802368164, Test Loss=3.6419904232025146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1007/10000 [00:37<11:31, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.0614112615585327, Test Loss=1.2852228879928589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2005/10000 [01:14<12:07, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.124786615371704, Test Loss=1.388229250907898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3004/10000 [01:50<14:47,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.0683794021606445, Test Loss=1.423374891281128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4003/10000 [02:43<15:23,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.9192647337913513, Test Loss=1.3892085552215576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5006/10000 [03:23<09:34,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.7025317549705505, Test Loss=1.3223774433135986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6005/10000 [04:00<05:53, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.5251529812812805, Test Loss=1.1339017152786255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7006/10000 [04:39<05:07,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.4093388617038727, Test Loss=1.0924144983291626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8006/10000 [05:19<03:33,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.2749190926551819, Test Loss=1.002109169960022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9004/10000 [05:58<01:43,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.2245638519525528, Test Loss=1.238279938697815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:38<00:00, 25.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=0.14825193583965302, Test Loss=1.3029029369354248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparamters\n",
    "num_iterations = 10000\n",
    "report_every_n = 1000\n",
    "eval_size = 100\n",
    "block_size = 16\n",
    "batch_size = 32\n",
    "embed_dim = 16\n",
    "num_heads = 8\n",
    "num_decoder_layers = 4\n",
    "\n",
    "model = SimpleGPT(vocab_size, embed_dim, block_size, num_heads, num_decoder_layers)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=stoi['.'])\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "dataset = ModularArithmeticDataset(block_size, eval_size)\n",
    "dataloader = DataLoader(dataset, batch_size)\n",
    "\n",
    "@torch.no_grad\n",
    "def estimate_loss():\n",
    "    losses = torch.zeros(eval_size)\n",
    "    model.eval()\n",
    "    dataset.validation_mode()\n",
    "    for i in range(eval_size):\n",
    "        inputs, targets = next(iter(dataloader))\n",
    "        logits = model(inputs)\n",
    "        B, T, C = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    dataset.training_mode()\n",
    "    return losses.mean()\n",
    "\n",
    "for i in tqdm(range(num_iterations)):\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = next(iter(dataloader))\n",
    "    logits = model(inputs)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % report_every_n == 0 or i == num_iterations - 1:\n",
    "        print(f\"Train Loss={loss.item()}, Test Loss={estimate_loss()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<1+1=2>\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, input):\n",
    "    input = torch.tensor([encode(input)])\n",
    "    while True:\n",
    "        logits = model(input)[:,-1,:]\n",
    "        prob = F.softmax(logits, dim=-1)\n",
    "        pred = torch.multinomial(prob, num_samples=1)\n",
    "        input = torch.cat((input, pred), dim=1)\n",
    "        if pred.item() == stoi['>']:\n",
    "            break\n",
    "    return decode(input[0].tolist())\n",
    "\n",
    "\n",
    "print(generate(model, \"<1+1=\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
