{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Shakespeare Language Model\n",
    "\n",
    "GPT-like language model that is trained on the tinyshakespeare dataset. This notebook was written while following Karpathy's 'Let's build GPT' vide. The only notable difference is the use of SentencePiece tokenization instead of a character level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  8368k      0 --:--:-- --:--:-- --:--:-- 8443k\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from zeptogpt.model import SimpleGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tinyshakespeare') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/tinyshakespeare\n",
      "  input_format: \n",
      "  model_prefix: models/shakespeare_tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: \n",
      "\n",
      "  user_defined_symbols: \n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/tinyshakespeare\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: \n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108171\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563798\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33675 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25671\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25671 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12645 obj=11.7019 num_tokens=52717 num_tokens/piece=4.169\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10772 obj=9.46046 num_tokens=53065 num_tokens/piece=4.9262\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8077 obj=9.47102 num_tokens=56344 num_tokens/piece=6.97586\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8069 obj=9.43248 num_tokens=56395 num_tokens/piece=6.98909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6049 obj=9.62524 num_tokens=61716 num_tokens/piece=10.2027\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6049 obj=9.58053 num_tokens=61712 num_tokens/piece=10.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4536 obj=9.85482 num_tokens=68025 num_tokens/piece=14.9967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4536 obj=9.80345 num_tokens=68028 num_tokens/piece=14.9974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3402 obj=10.137 num_tokens=74970 num_tokens/piece=22.037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3402 obj=10.0857 num_tokens=74971 num_tokens/piece=22.0373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2551 obj=10.4554 num_tokens=81942 num_tokens/piece=32.1215\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2551 obj=10.4013 num_tokens=81951 num_tokens/piece=32.125\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1913 obj=10.8732 num_tokens=89431 num_tokens/piece=46.7491\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1913 obj=10.8064 num_tokens=89440 num_tokens/piece=46.7538\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1434 obj=11.3422 num_tokens=96817 num_tokens/piece=67.5153\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1434 obj=11.2617 num_tokens=96830 num_tokens/piece=67.5244\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=11.8572 num_tokens=103715 num_tokens/piece=94.2864\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=11.7728 num_tokens=103744 num_tokens/piece=94.3127\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/shakespeare_tokenizer_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/shakespeare_tokenizer_model.vocab\n",
      "trainer_interface.cc(707) LOG(WARNING) The piece [\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n",
      "] contains escaped characters that break the format of models/shakespeare_tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='data/tinyshakespeare',\n",
    "                               model_prefix='models/shakespeare_tokenizer_model',\n",
    "                               vocab_size=1000,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='unigram',\n",
    "                               remove_extra_whitespaces=False,\n",
    "                               user_defined_symbols=[\"\\n\", \"\\r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/shakespeare_tokenizer_model.model')\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  3, 175,  13,  66, 610,  26,  27, 200],\n",
       "         [ 97, 128,  10,   5,  77,  11,  46, 109],\n",
       "         [ 39,  16,  12, 709,  30,   3,   3, 191],\n",
       "         [101, 182,  20, 242,   5,  94, 388, 119]]),\n",
       " tensor([[175,  13,  66, 610,  26,  27, 200,  60],\n",
       "         [128,  10,   5,  77,  11,  46, 109, 130],\n",
       "         [ 16,  12, 709,  30,   3,   3, 191,  57],\n",
       "         [182,  20, 242,   5,  94, 388, 119,  36]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(sp.encode(text))\n",
    "\n",
    "traindata = data[:int(0.9 * len(data))]\n",
    "testdata = data[int(0.9 * len(data)):]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(data, device, batch_size, block_size):\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "get_batch(traindata, 'cpu', 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class MultiheadedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, bias=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(embed_dim, 3 * embed_dim, bias=bias)\n",
    "        self.c_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x): # (B, T, C=embed_dim)\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.embed_dim, dim=2)\n",
    "        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiheadedSelfAttention(embed_dim, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(nn.Linear(embed_dim, 4*embed_dim), nn.GELU(approximate='tanh'), nn.Linear(4*embed_dim, embed_dim), nn.Dropout())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outs = inputs + self.attn(self.ln1(inputs))\n",
    "        outs = outs + self.mlp(self.ln2(outs))\n",
    "        return outs\n",
    "\n",
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, num_heads, num_decoder_layers):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb_table = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, embed_dim)\n",
    "        self.decoder_blocks = nn.Sequential(*[DecoderBlock(embed_dim, num_heads) for _ in range(num_decoder_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, inputs): # inputs: (B, T)\n",
    "        B, T = inputs.shape\n",
    "        tok_embed = self.tok_emb_table(inputs) # (B, T) -> (B, T, C=embed_dim)\n",
    "        pos_embed = self.pos_emb_table(torch.arange(T, device=inputs.device)) # (T, C=embed_dim)\n",
    "        x = tok_embed + pos_embed  # (B, T, C)\n",
    "        x = self.decoder_blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, C) -> (B, T, vocab_size)\n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def generate(self, context, num_tokens): # context: (1, T)\n",
    "        for _ in range(num_tokens):\n",
    "            logits = self(context[:, -self.block_size:])[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context = torch.cat((context, next_token), dim=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGPT(\n",
      "  (tok_emb_table): Embedding(1000, 32)\n",
      "  (pos_emb_table): Embedding(8, 32)\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): DecoderBlock(\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadedSelfAttention(\n",
      "        (c_attn): Linear(in_features=32, out_features=96, bias=True)\n",
      "        (c_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='tanh')\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadedSelfAttention(\n",
      "        (c_attn): Linear(in_features=32, out_features=96, bias=True)\n",
      "        (c_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='tanh')\n",
      "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=32, out_features=1000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dynamo is not supported on Python 3.12+",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     19\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mestimate_loss\u001b[39m(dataset):\n\u001b[1;32m     24\u001b[0m     losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(eval_iters)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/__init__.py:1801\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;66;03m# Temporary until we get proper support for python 3.12\u001b[39;00m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[0;32m-> 1801\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDynamo is not supported on Python 3.12+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;66;03m# Decorator mode\u001b[39;00m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dynamo is not supported on Python 3.12+"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from tqdm import tqdm\n",
    "\n",
    "embed_dims = 32\n",
    "num_heads = 4\n",
    "num_decoder_layers = 2\n",
    "eval_iters = 100\n",
    "eval_interval = 1000\n",
    "num_training_iters = 10000\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SimpleGPT(vocab_size, embed_dims, block_size, num_heads, num_decoder_layers)\n",
    "model.to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "@torch.no_grad\n",
    "def estimate_loss(dataset):\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    model.eval()\n",
    "    for i in range(eval_iters):\n",
    "        inputs, targets = get_batch(dataset, device, batch_size, block_size)\n",
    "        logits = model(inputs)\n",
    "        B, T, C = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_training_iters)):\n",
    "    inputs, targets = get_batch(traindata, device, batch_size, block_size)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(inputs)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % eval_interval == 0 or i == num_training_iters - 1:\n",
    "        print(f\"Train Loss={estimate_loss(traindata)} Test Loss={estimate_loss(testdata)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sowtlall atimeked and three to the faithements,\n",
      "Masn you cannot I but see'chent-tyome qulandth to have heavens and us:\n",
      "To make my punro\n",
      "Eieather:\n",
      "Me proveself, nurse doth o against's bad-bing.\n",
      "\n",
      "RICULET:\n",
      "That more strab of\n",
      "Linkdries the king\n",
      "Whis or Iy.\n",
      "\n",
      "OF OR:\n",
      "RIAell in his wifeels, you't will be for rembs.\n",
      "\n",
      "ESCALUS:\n",
      "I hast to set and\n",
      "AR: answer, that go; or she, thou shalt soon lety\n",
      "I not nothing be seempendan\n",
      "Now:\n",
      "I speak:\n",
      "Now wordly make'd, my daughter to themer,\n",
      "Neaster: sebiceinpe what,\n",
      "Splastutonceal me for most likes's kind and,\n",
      "Thant take the state rugoedfulper stillgickerk's, to labarvss of York; must to my lord:\n",
      "Whithing fiefast years that for,dthirs oby,\n",
      "Thou one the shrtatitbomed, that that lochly better.\n",
      "To cruforship, play.\n",
      "Mel you bid that it be here, howtimeg and not now.\n",
      "The\n",
      "Ant\n",
      "MO:\n",
      "My you, such die tend wit, but have give of this abasse; see the\n",
      "And mother mech to reprimt take point and semstied dat mytic country, jill?\n",
      "\n",
      "Bus lieleten us you too\n",
      "Ad, let thy shath,\n",
      "That Bant upon her respectd feence,--\n",
      "I am knowty: scervors upon Aapo that name, to my subject'd's now, andeign you born,\n",
      "And 'tis their the ha to propslial chiant letd,\n",
      "Thi twenty if strength that unomikss deparachate,\n",
      "Fow at Mant long?\n",
      "OR:\n",
      "As God, to the people:\n",
      "Naert, ere\n",
      "N:\n",
      "O:, sir, Repels,\n",
      "Comes shall' the grace.\n",
      "The prince a purdunds,--\n",
      "Hi keep mercy.\n",
      "\n",
      "MENENRIUS:\n",
      "And gentle Gloucester's me all seas perdkbemence. for propohension.\n",
      "\n",
      "ARIUS:\n",
      "I.\n",
      "Yay hopes's dead: I would'll said again;\n",
      "E myNE:\n",
      "SurXman 'tis rofity me\n",
      "I be queen the subject the stenious?\n",
      "Nost you;\n",
      "Opost me, and the hour conhatpe,\n",
      "And III:\n",
      "thy tain prodste spe.\n",
      "\n",
      "K poison, let their she do you yet you should is come,\n",
      "The confess Lulisest deservear lie wocch-cr never Come:\n",
      "I des give sunp.\n",
      "PSAR:\n",
      "Shat, bring ndread:\n",
      "Mir have takech uponer to us: thee to wonder impanting,\n",
      "Tain gerity\n",
      "To-pan Rome she sayver int in the battle done.\n",
      "\n",
      "TheUn fool, grace my lady;\n",
      "A these my lordI can trar still that follow'd\n",
      "Yhisless joy her plens.\n",
      "\n",
      "That peace the rest Bolingbroke thou but\n",
      "Ock\n",
      "To break his fault should wew of great frorash to my hat scer of his hand\n",
      "mis came, but not death, and speak: me thous\n",
      "MOr:\n",
      "Mat thou Bianca that she need, and my lord.\n",
      "\n",
      "No make will::\n",
      "hy, by imp rebepemrilectted meable Thou right\n",
      "I brother out exqumile of mp\n"
     ]
    }
   ],
   "source": [
    "print(sp.decode(model.generate(torch.ones((1,1), dtype=torch.long, device = device) * 80, 1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
